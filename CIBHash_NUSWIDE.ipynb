{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_x4DnyoQ41y"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle\n",
        "# import json\n",
        "# 这里我使用了自己上传的Kaggle数据集，方便下载：isdw134/nuswide21, 需要者可自行替换：https://www.kaggle.com/datasets/isdw134/nuswide21\n",
        "# token = {\"username\":\"\",\"key\":\"\"}\n",
        "# with open('/content/kaggle.json', 'w') as file:\n",
        "#  json.dump(token, file)\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp /content/kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle config set -n path -v /content\n",
        "\n",
        "# !kaggle datasets download -d isdw134/nuswide21\n",
        "\n",
        "# !unzip datasets/isdw134/nuswide21/nuswide21.zip -d /content/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCcHw_0IMLWO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "np.random.seed(0)\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiMDkJ0sMNY4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import statistics as stat\n",
        "import sys\n",
        "\n",
        "class Logger(object):\n",
        "\n",
        "    def __init__(self, log_path, on=True):\n",
        "        self.log_path = log_path\n",
        "        self.on = False\n",
        "\n",
        "        if self.on:\n",
        "            while os.path.isfile(self.log_path):\n",
        "                self.log_path += '+'\n",
        "\n",
        "    def log(self, string, newline=True):\n",
        "        if self.on:\n",
        "            with open(self.log_path, 'a') as logf:\n",
        "                logf.write(string)\n",
        "                if newline: logf.write('\\n')\n",
        "\n",
        "        sys.stdout.write(string)\n",
        "        if newline: sys.stdout.write('\\n')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def log_perfs(self, perfs):\n",
        "        valid_perfs = [perf for perf in perfs if not math.isinf(perf)]\n",
        "        best_perf = max(valid_perfs)\n",
        "        self.log('-' * 89)\n",
        "        self.log('%d perfs: %s' % (len(perfs), str(perfs)))\n",
        "        self.log('perf max: %g' % best_perf)\n",
        "        self.log('perf min: %g' % min(valid_perfs))\n",
        "        self.log('perf avg: %g' % stat.mean(valid_perfs))\n",
        "        self.log('perf std: %g' % (stat.stdev(valid_perfs)\n",
        "                                     if len(valid_perfs) > 1 else 0.0))\n",
        "        self.log('(excluded %d out of %d runs that produced -inf)' %\n",
        "                 (len(perfs) - len(valid_perfs), len(perfs)))\n",
        "        self.log('-' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnDJd9gDMOlA"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compress(train, test, encode_discrete, device):\n",
        "    retrievalB = list([])\n",
        "    retrievalL = list([])\n",
        "    for batch_step, (data, target) in enumerate(train):\n",
        "        var_data = Variable(data.to(device))\n",
        "        code = encode_discrete(var_data)\n",
        "        retrievalB.extend(code.cpu().data.numpy())\n",
        "        retrievalL.extend(target)\n",
        "\n",
        "    queryB = list([])\n",
        "    queryL = list([])\n",
        "    for batch_step, (data, target) in enumerate(test):\n",
        "        var_data = Variable(data.to(device))\n",
        "        code = encode_discrete(var_data)\n",
        "        queryB.extend(code.cpu().data.numpy())\n",
        "        queryL.extend(target)\n",
        "\n",
        "    retrievalB = np.array(retrievalB)\n",
        "    retrievalL = np.stack(retrievalL)\n",
        "\n",
        "    queryB = np.array(queryB)\n",
        "    queryL = np.stack(queryL)\n",
        "    return retrievalB, retrievalL, queryB, queryL\n",
        "\n",
        "\n",
        "def calculate_hamming(B1, B2):\n",
        "    \"\"\"\n",
        "    :param B1:  vector [n]\n",
        "    :param B2:  vector [r*n]\n",
        "    :return: hamming distance [r]\n",
        "    \"\"\"\n",
        "    q = B2.shape[1] # max inner product value\n",
        "    distH = 0.5 * (q - np.dot(B1, B2.transpose()))\n",
        "    return distH\n",
        "\n",
        "\n",
        "def calculate_top_map(qB, rB, queryL, retrievalL, topk):\n",
        "    \"\"\"\n",
        "    :param qB: {-1,+1}^{mxq} query bits\n",
        "    :param rB: {-1,+1}^{nxq} retrieval bits\n",
        "    :param queryL: {0,1}^{mxl} query label\n",
        "    :param retrievalL: {0,1}^{nxl} retrieval label\n",
        "    :param topk:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_query = queryL.shape[0]\n",
        "    topkmap = 0\n",
        "    for iter in range(num_query):\n",
        "        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n",
        "        hamm = calculate_hamming(qB[iter, :], rB)\n",
        "        ind = np.argsort(hamm)\n",
        "        gnd = gnd[ind] # reorder gnd\n",
        "\n",
        "        tgnd = gnd[0:topk]\n",
        "        tsum = int(np.sum(tgnd))\n",
        "        if tsum == 0:\n",
        "            continue\n",
        "        count = np.linspace(1, tsum, tsum)\n",
        "\n",
        "        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n",
        "        topkmap_ = np.mean(count / (tindex))\n",
        "        # print(topkmap_)\n",
        "        topkmap = topkmap + topkmap_\n",
        "    topkmap = topkmap / num_query\n",
        "    return topkmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXZTvLGLMQYc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# from utils.gaussian_blur import GaussianBlur\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.load_datasets()\n",
        "\n",
        "        # setup dataTransform\n",
        "        color_jitter = transforms.ColorJitter(0.4,0.4,0.4,0.1)\n",
        "        self.train_transforms = transforms.Compose([transforms.RandomResizedCrop(size = 224,scale=(0.5, 1.0)),\n",
        "                                            transforms.RandomHorizontalFlip(),\n",
        "                                            transforms.RandomApply([color_jitter], p = 0.7),\n",
        "                                            transforms.RandomGrayscale(p  = 0.2),\n",
        "                                            GaussianBlur(3),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                            ])\n",
        "        self.test_transforms = transforms.Compose([\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.test_cifar10_transforms = transforms.Compose([\n",
        "                                            transforms.Resize((224, 224)),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def load_datasets(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_loaders(self, batch_size, num_workers, shuffle_train=False,\n",
        "                    get_test=True):\n",
        "        train_dataset = MyTrainDataset(self.X_train, self.Y_train, self.train_transforms)\n",
        "\n",
        "        if(self.dataset == 'cifar10'):\n",
        "            val_dataset = MyTestDataset(self.X_val, self.Y_val, self.test_cifar10_transforms, self.dataset)\n",
        "            test_dataset = MyTestDataset(self.X_test, self.Y_test, self.test_cifar10_transforms, self.dataset)\n",
        "            database_dataset = MyTestDataset(self.X_database, self.Y_database, self.test_cifar10_transforms, self.dataset)\n",
        "        else:\n",
        "            val_dataset = MyTestDataset(self.X_val, self.Y_val, self.test_transforms, self.dataset)\n",
        "            test_dataset = MyTestDataset(self.X_test, self.Y_test, self.test_transforms, self.dataset)\n",
        "            database_dataset = MyTestDataset(self.X_database, self.Y_database, self.test_transforms, self.dataset)\n",
        "\n",
        "        # DataLoader\n",
        "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
        "                                                shuffle=shuffle_train,\n",
        "                                                num_workers=num_workers)\n",
        "\n",
        "        val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=num_workers)\n",
        "\n",
        "        test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=num_workers) if get_test else None\n",
        "\n",
        "        database_loader = DataLoader(dataset=database_dataset, batch_size=batch_size,\n",
        "                                                    shuffle=False,\n",
        "                                                    num_workers=num_workers)\n",
        "\n",
        "        return train_loader, val_loader, test_loader, database_loader\n",
        "\n",
        "\n",
        "class LabeledData(Data):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__(dataset=dataset)\n",
        "\n",
        "    def load_datasets(self):\n",
        "        if(self.dataset == 'cifar10'):\n",
        "            self.topK = 1000\n",
        "            self.X_train, self.Y_train, self.X_val, self.Y_val, self.X_test, self.Y_test, self.X_database, self.Y_database = get_cifar()\n",
        "        elif(self.dataset == 'nuswide'):\n",
        "            self.topK = 5000\n",
        "            self.X_train, self.Y_train, self.X_val, self.Y_val, self.X_test, self.Y_test, self.X_database, self.Y_database = get_nuswide()\n",
        "        else:\n",
        "            raise NotImplementedError(\"Please use the right dataset!\")\n",
        "\n",
        "class MyTrainDataset(Dataset):\n",
        "    def __init__(self,data,labels, transform):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform  = transform\n",
        "    def __getitem__(self, index):\n",
        "        pilImg = Image.fromarray(self.data[index])\n",
        "        imgi = self.transform(pilImg)\n",
        "        imgj = self.transform(pilImg)\n",
        "        return (imgi, imgj, self.labels[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class MyTestDataset(Dataset):\n",
        "    def __init__(self,data,labels, transform,dataset):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform  = transform\n",
        "        self.dataset = dataset\n",
        "    def __getitem__(self, index):\n",
        "        if self.dataset == 'cifar10':\n",
        "            pilImg = Image.fromarray(self.data[index])\n",
        "            return (self.transform(pilImg),self.labels[index])\n",
        "        else:\n",
        "            return (self.transform(self.data[index]),self.labels[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def get_cifar():\n",
        "    # Dataset\n",
        "    train_dataset = dsets.CIFAR10(root='./data/cifar10/',\n",
        "                                train=True,\n",
        "                                download=True)\n",
        "\n",
        "    test_dataset = dsets.CIFAR10(root='./data/cifar10/',\n",
        "                                train=False\n",
        "                                )\n",
        "\n",
        "    database_dataset = dsets.CIFAR10(root='./data/cifar10/',\n",
        "                                    train=True\n",
        "                                    )\n",
        "\n",
        "\n",
        "    # train with 5000 images\n",
        "    X = train_dataset.data\n",
        "    L = np.array(train_dataset.targets)\n",
        "\n",
        "    first = True\n",
        "    for label in range(10):\n",
        "        index = np.where(L == label)[0]\n",
        "        N = index.shape[0]\n",
        "        prem = np.random.permutation(N)\n",
        "        index = index[prem]\n",
        "\n",
        "        data = X[index[0:500]]\n",
        "        labels = L[index[0: 500]]\n",
        "        if first:\n",
        "            Y_train = labels\n",
        "            X_train = data\n",
        "        else:\n",
        "            Y_train = np.concatenate((Y_train, labels))\n",
        "            X_train = np.concatenate((X_train, data))\n",
        "        first = False\n",
        "\n",
        "    Y_train = np.eye(10)[Y_train]\n",
        "\n",
        "\n",
        "    idxs = list(range(len(test_dataset.data)))\n",
        "    np.random.shuffle(idxs)\n",
        "    test_data = np.array(test_dataset.data)\n",
        "    test_tragets = np.array(test_dataset.targets)\n",
        "\n",
        "    X_val = test_data[idxs[:5000]]\n",
        "    Y_val = np.eye(10)[test_tragets[idxs[:5000]]]\n",
        "\n",
        "    X_test = test_data[idxs[5000:]]\n",
        "    Y_test = np.eye(10)[test_tragets[idxs[5000:]]]\n",
        "\n",
        "\n",
        "    X_database = database_dataset.data\n",
        "    Y_database = np.eye(10)[database_dataset.targets]\n",
        "\n",
        "    return X_train, Y_train, X_val, Y_val, X_test, Y_test, X_database, Y_database\n",
        "\n",
        "def download_images(urls_file, output_folder):\n",
        "    with open(urls_file, 'r') as f:\n",
        "        next(f)  # Skip the header line\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 3:  # Ensure at least three elements\n",
        "                photo_file, photo_id, url = parts[0], parts[1], parts[2]\n",
        "                image_folder = os.path.join(output_folder, os.path.dirname(photo_file))\n",
        "                os.makedirs(image_folder, exist_ok=True)\n",
        "                image_path = os.path.join(image_folder, os.path.basename(photo_file))\n",
        "                try:\n",
        "                    if url != 'null':\n",
        "                        urllib.request.urlretrieve(url, image_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading image {photo_id}: {e}\")\n",
        "            else:\n",
        "                print(f\"Ignore malformed line: {line.strip()}\")\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class NUSWIDEDataset(Dataset):\n",
        "    def __init__(self, data_dir, filenames, labels, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.filenames = filenames\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.data_dir, self.filenames[index])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def get_nuswide():\n",
        "    # Read database.txt, test.txt, and train.txt\n",
        "    def read_txt(file_path):\n",
        "        filenames = []\n",
        "        labels = []\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                parts = line.strip().split()\n",
        "                filenames.append(parts[0])\n",
        "                labels.append(list(map(int, parts[1:])))\n",
        "        return filenames, labels\n",
        "\n",
        "    database_dir = 'data/NUS-WIDE'\n",
        "    train_filenames, train_labels = read_txt('data/NUS-WIDE/train.txt')\n",
        "    test_filenames, test_labels = read_txt('data/NUS-WIDE/test.txt')\n",
        "    database_filenames, database_labels = read_txt('data/NUS-WIDE/database.txt')\n",
        "\n",
        "    # Convert labels to numpy arrays\n",
        "    train_labels = np.array(train_labels)\n",
        "    test_labels = np.array(test_labels)\n",
        "    database_labels = np.array(database_labels)\n",
        "\n",
        "    def load_images(data_dir, filenames, target_size=(224, 224)):\n",
        "        images = []\n",
        "        for filename in tqdm(filenames, desc=\"Loading images\"):\n",
        "            img_path = os.path.join(data_dir, filename)\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            # Resize image to target size\n",
        "            image = ImageOps.fit(image, target_size, Image.ANTIALIAS)\n",
        "            images.append(np.array(image))\n",
        "        return np.array(images)\n",
        "\n",
        "    X_train = load_images(database_dir, train_filenames)\n",
        "    X_val = load_images(database_dir, test_filenames)\n",
        "    X_test = load_images(database_dir, test_filenames)\n",
        "    X_database = load_images(database_dir, database_filenames)\n",
        "\n",
        "    return X_train, train_labels, X_val, test_labels, X_test, test_labels, X_database, database_labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5J2WOhoIC_w"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import sklearn\n",
        "import argparse\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from datetime import timedelta\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.patheffects as pe\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "from sklearn.datasets import load_digits\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "# from utils.logger import Logger\n",
        "# from utils.data import LabeledData\n",
        "# from utils.evaluation import calculate_hamming\n",
        "# from utils.evaluation import compress, calculate_top_map\n",
        "\n",
        "class Base_Model(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        self.data = LabeledData(self.hparams.dataset)\n",
        "\n",
        "    def get_hparams_grid(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def define_parameters(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run_training_sessions(self):\n",
        "        logger = Logger(self.hparams.model_path + '.log', on=True)\n",
        "        val_perfs = []\n",
        "        best_val_perf = float('-inf')\n",
        "        start = timer()\n",
        "        random.seed(self.hparams.seed)  # For reproducible random runs\n",
        "\n",
        "        for run_num in range(1, self.hparams.num_runs + 1):\n",
        "            state_dict, val_perf = self.run_training_session(run_num, logger)\n",
        "            val_perfs.append(val_perf)\n",
        "\n",
        "            if val_perf > best_val_perf:\n",
        "                best_val_perf = val_perf\n",
        "                logger.log('----New best {:8.4f}, saving'.format(val_perf))\n",
        "                torch.save({'hparams': self.hparams,\n",
        "                            'state_dict': state_dict}, self.hparams.model_path)\n",
        "\n",
        "        logger.log('Time: %s' % str(timedelta(seconds=round(timer() - start))))\n",
        "        self.load()\n",
        "        if self.hparams.num_runs > 1:\n",
        "            logger.log_perfs(val_perfs)\n",
        "            logger.log('best hparams: ' + self.flag_hparams())\n",
        "\n",
        "        val_perf, test_perf = self.run_test()\n",
        "        logger.log('Val:  {:8.4f}'.format(val_perf))\n",
        "        logger.log('Test: {:8.4f}'.format(test_perf))\n",
        "\n",
        "    def run_training_session(self, run_num, logger):\n",
        "        self.train()\n",
        "\n",
        "        # Scramble hyperparameters if number of runs is greater than 1.\n",
        "        if self.hparams.num_runs > 1:\n",
        "            logger.log('RANDOM RUN: %d/%d' % (run_num, self.hparams.num_runs))\n",
        "            for hparam, values in self.get_hparams_grid().items():\n",
        "                assert hasattr(self.hparams, hparam)\n",
        "                self.hparams.__dict__[hparam] = random.choice(values)\n",
        "\n",
        "        random.seed(self.hparams.seed)\n",
        "        torch.manual_seed(self.hparams.seed)\n",
        "\n",
        "        self.define_parameters()\n",
        "\n",
        "        # if encode_length is 16, then al least 80 epochs!\n",
        "        if self.hparams.encode_length == 16:\n",
        "            self.hparams.epochs = max(80, self.hparams.epochs)\n",
        "\n",
        "        logger.log('hparams: %s' % self.flag_hparams())\n",
        "\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        self.to(device)\n",
        "\n",
        "        optimizer = self.configure_optimizers()\n",
        "        train_loader, val_loader, _, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=True, get_test=False)\n",
        "        best_val_perf = float('-inf')\n",
        "        best_state_dict = None\n",
        "        bad_epochs = 0\n",
        "\n",
        "        try:\n",
        "            for epoch in range(1, self.hparams.epochs + 1):\n",
        "                forward_sum = {}\n",
        "                num_steps = 0\n",
        "                for batch_num, batch in enumerate(train_loader):\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    imgi, imgj, _ = batch\n",
        "                    imgi = imgi.to(device)\n",
        "                    imgj = imgj.to(device)\n",
        "\n",
        "                    forward = self.forward(imgi, imgj, device)\n",
        "\n",
        "                    for key in forward:\n",
        "                        if key in forward_sum:\n",
        "                            forward_sum[key] += forward[key]\n",
        "                        else:\n",
        "                            forward_sum[key] = forward[key]\n",
        "                    num_steps += 1\n",
        "\n",
        "                    if math.isnan(forward_sum['loss']):\n",
        "                        logger.log('Stopping epoch because loss is NaN')\n",
        "                        break\n",
        "\n",
        "                    forward['loss'].backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                if math.isnan(forward_sum['loss']):\n",
        "                    logger.log('Stopping training session because loss is NaN')\n",
        "                    break\n",
        "\n",
        "                logger.log('End of epoch {:3d}'.format(epoch), False)\n",
        "                logger.log(' '.join([' | {:s} {:8.4f}'.format(\n",
        "                    key, forward_sum[key] / num_steps)\n",
        "                                     for key in forward_sum]), True)\n",
        "\n",
        "                if epoch % self.hparams.validate_frequency == 0:\n",
        "                    print('evaluating...')\n",
        "                    val_perf = self.evaluate(database_loader, val_loader, self.data.topK, device)\n",
        "                    logger.log(' | val perf {:8.4f}'.format(val_perf), False)\n",
        "\n",
        "                    if val_perf > best_val_perf:\n",
        "                        best_val_perf = val_perf\n",
        "                        bad_epochs = 0\n",
        "                        logger.log('\\t\\t*Best model so far, deep copying*')\n",
        "                        best_state_dict = deepcopy(self.state_dict())\n",
        "                    else:\n",
        "                        bad_epochs += 1\n",
        "                        logger.log('\\t\\tBad epoch %d' % bad_epochs)\n",
        "\n",
        "                    if bad_epochs > self.hparams.num_bad_epochs:\n",
        "                        break\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.log('-' * 89)\n",
        "            logger.log('Exiting from training early')\n",
        "\n",
        "        return best_state_dict, best_val_perf\n",
        "\n",
        "    def evaluate(self, database_loader, val_loader, topK, device):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            retrievalB, retrievalL, queryB, queryL = compress(database_loader, val_loader, self.encode_discrete, device)\n",
        "            result = calculate_top_map(qB=queryB, rB=retrievalB, queryL=queryL, retrievalL=retrievalL, topk=topK)\n",
        "        self.train()\n",
        "        return result\n",
        "\n",
        "    def load(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        checkpoint = torch.load(self.hparams.model_path) if self.hparams.cuda \\\n",
        "                     else torch.load(self.hparams.model_path,\n",
        "                                     map_location=torch.device('cpu'))\n",
        "        if checkpoint['hparams'].cuda and not self.hparams.cuda:\n",
        "            checkpoint['hparams'].cuda = False\n",
        "        self.hparams = checkpoint['hparams']\n",
        "        self.define_parameters()\n",
        "        self.load_state_dict(checkpoint['state_dict'])\n",
        "        self.to(device)\n",
        "\n",
        "    def run_test(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        _, val_loader, test_loader, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        val_perf = self.evaluate(database_loader, val_loader, self.data.topK, device)\n",
        "        test_perf = self.evaluate(database_loader, test_loader, self.data.topK, device)\n",
        "        return val_perf, test_perf\n",
        "\n",
        "    def run_retrieval_case_study(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        query_idxs = [0,2,5]\n",
        "        X_database = self.data.X_database\n",
        "        X_test = self.data.X_test\n",
        "        X_case = torch.cat([self.data.test_cifar10_transforms(Image.fromarray(self.data.X_test[i])).unsqueeze(0) for i in query_idxs], dim=0)\n",
        "        _, val_loader, test_loader, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        # get hash codes\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            retrievalB = list([])\n",
        "            for batch_step, (data, target) in enumerate(database_loader):\n",
        "                var_data = Variable(data.to(device))\n",
        "                code = self.encode_discrete(var_data)\n",
        "                retrievalB.extend(code.cpu().data.numpy())\n",
        "\n",
        "            queryB = list([])\n",
        "            var_data = Variable(X_case.to(device))\n",
        "            code = self.encode_discrete(var_data)\n",
        "            queryB.extend(code.cpu().data.numpy())\n",
        "\n",
        "        retrievalB = np.array(retrievalB)\n",
        "        queryB = np.array(queryB)\n",
        "\n",
        "        # get top 10 index\n",
        "        top10_idx_list = []\n",
        "        for idx in range(queryB.shape[0]):\n",
        "            hamm = calculate_hamming(queryB[idx, :], retrievalB)\n",
        "            ind = list(np.argsort(hamm)[:10])\n",
        "            top10_idx_list.append(ind)\n",
        "\n",
        "        # plot results\n",
        "        fig = plt.figure(0, figsize = (5,1.2))\n",
        "        fig.clf()\n",
        "        gs = gridspec.GridSpec(queryB.shape[0], 12)\n",
        "        gs.update(wspace = 0.0001, hspace = 0.0001)\n",
        "        for i in range(queryB.shape[0]):\n",
        "            axes = plt.subplot(gs[i,0])\n",
        "            axes.imshow(X_test[query_idxs[i]])\n",
        "            axes.axis('off')\n",
        "\n",
        "            for j in range(0, 10):\n",
        "                axes = plt.subplot(gs[i, j+2])\n",
        "                axes.imshow(X_database[top10_idx_list[i][j]])\n",
        "                axes.axis('off')\n",
        "        fig.savefig(\"retrieval_case_study_{:d}bits.pdf\".format(self.hparams.encode_length), bbox_inches='tight', pad_inches=0.0)\n",
        "\n",
        "    def hash_code_visualization(self):\n",
        "        \"\"\"\n",
        "        cifar10 labels:\n",
        "        0: Airplane 1: Automobile 2: Bird 3: Cat 4: Deer\n",
        "        5: Dog 6: Frog 7: Horse 8: Ship 9: Truck\n",
        "        \"\"\"\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        _, _, test_loader, _ = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        retrievalB = list([])\n",
        "        retrievalL = list([])\n",
        "        for batch_step, (data, target) in enumerate(test_loader):\n",
        "            var_data = Variable(data.to(device))\n",
        "            code = self.encode_discrete(var_data)\n",
        "            retrievalB.extend(code.cpu().data.numpy())\n",
        "            retrievalL.extend(target.cpu().data.numpy())\n",
        "\n",
        "        hash_codes = np.array(retrievalB)\n",
        "        _, labels = np.where(np.array(retrievalL) == 1)\n",
        "        labels_ticks = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "\n",
        "        # TSN\n",
        "        mapper = TSNE(perplexity=30).fit_transform(hash_codes)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(mapper[:,0], mapper[:,1], lw=0, s=20, c=labels.astype(np.int), cmap='Spectral')\n",
        "        # cbar = plt.colorbar(boundaries=np.arange(11)-0.5, fraction=0.046, pad=0.04)\n",
        "        # cbar.set_ticks(np.arange(10))\n",
        "        # cbar.set_ticklabels(labels_ticks)\n",
        "\n",
        "        # Add the labels for each digit.\n",
        "        for i in range(10):\n",
        "            # Position of each label.\n",
        "            xtext, ytext = np.median(mapper[labels == i, :], axis=0)\n",
        "            txt = plt.text(xtext, ytext, str(i), fontsize=24)\n",
        "            txt.set_path_effects([pe.Stroke(linewidth=5, foreground=\"w\"), pe.Normal()])\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.gcf().tight_layout()\n",
        "        plt.savefig('Ours_hash_codes_visulization_{:d}bits.pdf'.format(self.hparams.encode_length), bbox_inches='tight', pad_inches=0.0)\n",
        "\n",
        "    def flag_hparams(self):\n",
        "        flags = '%s' % (self.hparams.model_path)\n",
        "        for hparam in vars(self.hparams):\n",
        "            val = getattr(self.hparams, hparam)\n",
        "            if str(val) == 'False':\n",
        "                continue\n",
        "            elif str(val) == 'True':\n",
        "                flags += ' --%s' % (hparam)\n",
        "            elif str(hparam) in {'model_path', 'num_runs',\n",
        "                                 'num_workers'}:\n",
        "                continue\n",
        "            else:\n",
        "                flags += ' --%s %s' % (hparam, val)\n",
        "        return flags\n",
        "\n",
        "    @staticmethod\n",
        "    def get_general_hparams_grid():\n",
        "        grid = OrderedDict({\n",
        "            'seed': list(range(100000)),\n",
        "            'lr': [0.003, 0.001, 0.0003, 0.0001],\n",
        "            'batch_size': [64, 128, 256],\n",
        "            })\n",
        "        return grid\n",
        "\n",
        "    @staticmethod\n",
        "    def get_general_argparser():\n",
        "        parser = argparse.ArgumentParser()\n",
        "\n",
        "        parser.add_argument('model_path', type=str)\n",
        "        parser.add_argument('--train', action='store_true',\n",
        "                            help='train a model?')\n",
        "        parser.add_argument('-d', '--dataset', default = 'cifar10', type=str,\n",
        "                            help='dataset [%(default)s]')\n",
        "        parser.add_argument(\"-l\",\"--encode_length\", type = int, default=16,\n",
        "                            help = \"Number of bits of the hash code [%(default)d]\")\n",
        "        parser.add_argument(\"--lr\", default = 1e-3, type = float,\n",
        "                            help='initial learning rate [%(default)g]')\n",
        "        parser.add_argument(\"--batch_size\", default=64,type=int,\n",
        "                            help='batch size [%(default)d]')\n",
        "        parser.add_argument(\"-e\",\"--epochs\", default=60, type=int,\n",
        "                            help='max number of epochs [%(default)d]')\n",
        "        parser.add_argument('--cuda', action='store_true',\n",
        "                            help='use CUDA?')\n",
        "        parser.add_argument('--num_runs', type=int, default=1,\n",
        "                            help='num random runs (not random if 1) '\n",
        "                            '[%(default)d]')\n",
        "        parser.add_argument('--num_bad_epochs', type=int, default=6,\n",
        "                            help='num indulged bad epochs [%(default)d]')\n",
        "        parser.add_argument('--validate_frequency', type=int, default=20,\n",
        "                            help='validate every [%(default)d] epochs')\n",
        "        parser.add_argument('--num_workers', type=int, default=8,\n",
        "                            help='num dataloader workers [%(default)d]')\n",
        "        parser.add_argument('--seed', type=int, default=8888,\n",
        "                            help='random seed [%(default)d]')\n",
        "        parser.add_argument('--device', type=int, default=0,\n",
        "                            help='device of the gpu')\n",
        "\n",
        "\n",
        "        return parser\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vylyrlM2M0Y_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import sklearn\n",
        "import argparse\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from datetime import timedelta\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.patheffects as pe\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "from sklearn.datasets import load_digits\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "# from utils.logger import Logger\n",
        "# from utils.data import LabeledData\n",
        "# from utils.evaluation import calculate_hamming\n",
        "# from utils.evaluation import compress, calculate_top_map\n",
        "\n",
        "class Base_Model(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        self.data = LabeledData(self.hparams.dataset)\n",
        "\n",
        "    def get_hparams_grid(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def define_parameters(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run_training_sessions(self):\n",
        "        logger = Logger(self.hparams.model_path + '.log', on=True)\n",
        "        val_perfs = []\n",
        "        best_val_perf = float('-inf')\n",
        "        start = timer()\n",
        "        random.seed(self.hparams.seed)  # For reproducible random runs\n",
        "\n",
        "        for run_num in range(1, self.hparams.num_runs + 1):\n",
        "            state_dict, val_perf = self.run_training_session(run_num, logger)\n",
        "            val_perfs.append(val_perf)\n",
        "\n",
        "            if val_perf > best_val_perf:\n",
        "                best_val_perf = val_perf\n",
        "                logger.log('----New best {:8.4f}, saving'.format(val_perf))\n",
        "                torch.save({'hparams': self.hparams,\n",
        "                            'state_dict': state_dict}, self.hparams.model_path)\n",
        "\n",
        "        logger.log('Time: %s' % str(timedelta(seconds=round(timer() - start))))\n",
        "        self.load()\n",
        "        if self.hparams.num_runs > 1:\n",
        "            logger.log_perfs(val_perfs)\n",
        "            logger.log('best hparams: ' + self.flag_hparams())\n",
        "\n",
        "        val_perf, test_perf = self.run_test()\n",
        "        logger.log('Val:  {:8.4f}'.format(val_perf))\n",
        "        logger.log('Test: {:8.4f}'.format(test_perf))\n",
        "\n",
        "    def run_training_session(self, run_num, logger):\n",
        "        self.train()\n",
        "\n",
        "        # Scramble hyperparameters if number of runs is greater than 1.\n",
        "        if self.hparams.num_runs > 1:\n",
        "            logger.log('RANDOM RUN: %d/%d' % (run_num, self.hparams.num_runs))\n",
        "            for hparam, values in self.get_hparams_grid().items():\n",
        "                assert hasattr(self.hparams, hparam)\n",
        "                self.hparams.__dict__[hparam] = random.choice(values)\n",
        "\n",
        "        random.seed(self.hparams.seed)\n",
        "        torch.manual_seed(self.hparams.seed)\n",
        "\n",
        "        self.define_parameters()\n",
        "\n",
        "        # if encode_length is 16, then al least 80 epochs!\n",
        "        if self.hparams.encode_length == 16:\n",
        "            self.hparams.epochs = max(80, self.hparams.epochs)\n",
        "\n",
        "        logger.log('hparams: %s' % self.flag_hparams())\n",
        "\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        self.to(device)\n",
        "\n",
        "        optimizer = self.configure_optimizers()\n",
        "        train_loader, val_loader, _, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=True, get_test=False)\n",
        "        best_val_perf = float('-inf')\n",
        "        best_state_dict = None\n",
        "        bad_epochs = 0\n",
        "\n",
        "        try:\n",
        "            for epoch in range(1, self.hparams.epochs + 1):\n",
        "                forward_sum = {}\n",
        "                num_steps = 0\n",
        "                for batch_num, batch in enumerate(train_loader):\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    imgi, imgj, _ = batch\n",
        "                    imgi = imgi.to(device)\n",
        "                    imgj = imgj.to(device)\n",
        "\n",
        "                    forward = self.forward(imgi, imgj, device)\n",
        "\n",
        "                    for key in forward:\n",
        "                        if key in forward_sum:\n",
        "                            forward_sum[key] += forward[key]\n",
        "                        else:\n",
        "                            forward_sum[key] = forward[key]\n",
        "                    num_steps += 1\n",
        "\n",
        "                    if math.isnan(forward_sum['loss']):\n",
        "                        logger.log('Stopping epoch because loss is NaN')\n",
        "                        break\n",
        "\n",
        "                    forward['loss'].backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                if math.isnan(forward_sum['loss']):\n",
        "                    logger.log('Stopping training session because loss is NaN')\n",
        "                    break\n",
        "\n",
        "                logger.log('End of epoch {:3d}'.format(epoch), False)\n",
        "                logger.log(' '.join([' | {:s} {:8.4f}'.format(\n",
        "                    key, forward_sum[key] / num_steps)\n",
        "                                     for key in forward_sum]), True)\n",
        "\n",
        "                if epoch % self.hparams.validate_frequency == 0:\n",
        "                    print('evaluating...')\n",
        "                    val_perf = self.evaluate(database_loader, val_loader, self.data.topK, device)\n",
        "                    logger.log(' | val perf {:8.4f}'.format(val_perf), False)\n",
        "\n",
        "                    if val_perf > best_val_perf:\n",
        "                        best_val_perf = val_perf\n",
        "                        bad_epochs = 0\n",
        "                        logger.log('\\t\\t*Best model so far, deep copying*')\n",
        "                        best_state_dict = deepcopy(self.state_dict())\n",
        "                    else:\n",
        "                        bad_epochs += 1\n",
        "                        logger.log('\\t\\tBad epoch %d' % bad_epochs)\n",
        "\n",
        "                    if bad_epochs > self.hparams.num_bad_epochs:\n",
        "                        break\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.log('-' * 89)\n",
        "            logger.log('Exiting from training early')\n",
        "\n",
        "        return best_state_dict, best_val_perf\n",
        "\n",
        "    def evaluate(self, database_loader, val_loader, topK, device):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            retrievalB, retrievalL, queryB, queryL = compress(database_loader, val_loader, self.encode_discrete, device)\n",
        "            result = calculate_top_map(qB=queryB, rB=retrievalB, queryL=queryL, retrievalL=retrievalL, topk=topK)\n",
        "        self.train()\n",
        "        return result\n",
        "\n",
        "    def load(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        checkpoint = torch.load(self.hparams.model_path) if self.hparams.cuda \\\n",
        "                     else torch.load(self.hparams.model_path,\n",
        "                                     map_location=torch.device('cpu'))\n",
        "        if checkpoint['hparams'].cuda and not self.hparams.cuda:\n",
        "            checkpoint['hparams'].cuda = False\n",
        "        self.hparams = checkpoint['hparams']\n",
        "        self.define_parameters()\n",
        "        self.load_state_dict(checkpoint['state_dict'])\n",
        "        self.to(device)\n",
        "\n",
        "    def run_test(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        _, val_loader, test_loader, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        val_perf = self.evaluate(database_loader, val_loader, self.data.topK, device)\n",
        "        test_perf = self.evaluate(database_loader, test_loader, self.data.topK, device)\n",
        "        return val_perf, test_perf\n",
        "\n",
        "    def run_retrieval_case_study(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        query_idxs = [0,2,5]\n",
        "        X_database = self.data.X_database\n",
        "        X_test = self.data.X_test\n",
        "        X_case = torch.cat([self.data.test_cifar10_transforms(Image.fromarray(self.data.X_test[i])).unsqueeze(0) for i in query_idxs], dim=0)\n",
        "        _, val_loader, test_loader, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        # get hash codes\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            retrievalB = list([])\n",
        "            for batch_step, (data, target) in enumerate(database_loader):\n",
        "                var_data = Variable(data.to(device))\n",
        "                code = self.encode_discrete(var_data)\n",
        "                retrievalB.extend(code.cpu().data.numpy())\n",
        "\n",
        "            queryB = list([])\n",
        "            var_data = Variable(X_case.to(device))\n",
        "            code = self.encode_discrete(var_data)\n",
        "            queryB.extend(code.cpu().data.numpy())\n",
        "\n",
        "        retrievalB = np.array(retrievalB)\n",
        "        queryB = np.array(queryB)\n",
        "\n",
        "        # get top 10 index\n",
        "        top10_idx_list = []\n",
        "        for idx in range(queryB.shape[0]):\n",
        "            hamm = calculate_hamming(queryB[idx, :], retrievalB)\n",
        "            ind = list(np.argsort(hamm)[:10])\n",
        "            top10_idx_list.append(ind)\n",
        "\n",
        "        # plot results\n",
        "        fig = plt.figure(0, figsize = (5,1.2))\n",
        "        fig.clf()\n",
        "        gs = gridspec.GridSpec(queryB.shape[0], 12)\n",
        "        gs.update(wspace = 0.0001, hspace = 0.0001)\n",
        "        for i in range(queryB.shape[0]):\n",
        "            axes = plt.subplot(gs[i,0])\n",
        "            axes.imshow(X_test[query_idxs[i]])\n",
        "            axes.axis('off')\n",
        "\n",
        "            for j in range(0, 10):\n",
        "                axes = plt.subplot(gs[i, j+2])\n",
        "                axes.imshow(X_database[top10_idx_list[i][j]])\n",
        "                axes.axis('off')\n",
        "        fig.savefig(\"retrieval_case_study_{:d}bits.pdf\".format(self.hparams.encode_length), bbox_inches='tight', pad_inches=0.0)\n",
        "\n",
        "    def hash_code_visualization(self):\n",
        "        \"\"\"\n",
        "        cifar10 labels:\n",
        "        0: Airplane 1: Automobile 2: Bird 3: Cat 4: Deer\n",
        "        5: Dog 6: Frog 7: Horse 8: Ship 9: Truck\n",
        "        \"\"\"\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        _, _, test_loader, _ = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        retrievalB = list([])\n",
        "        retrievalL = list([])\n",
        "        for batch_step, (data, target) in enumerate(test_loader):\n",
        "            var_data = Variable(data.to(device))\n",
        "            code = self.encode_discrete(var_data)\n",
        "            retrievalB.extend(code.cpu().data.numpy())\n",
        "            retrievalL.extend(target.cpu().data.numpy())\n",
        "\n",
        "        hash_codes = np.array(retrievalB)\n",
        "        _, labels = np.where(np.array(retrievalL) == 1)\n",
        "        labels_ticks = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "\n",
        "        # TSN\n",
        "        mapper = TSNE(perplexity=30).fit_transform(hash_codes)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(mapper[:,0], mapper[:,1], lw=0, s=20, c=labels.astype(np.int), cmap='Spectral')\n",
        "        # cbar = plt.colorbar(boundaries=np.arange(11)-0.5, fraction=0.046, pad=0.04)\n",
        "        # cbar.set_ticks(np.arange(10))\n",
        "        # cbar.set_ticklabels(labels_ticks)\n",
        "\n",
        "        # Add the labels for each digit.\n",
        "        for i in range(10):\n",
        "            # Position of each label.\n",
        "            xtext, ytext = np.median(mapper[labels == i, :], axis=0)\n",
        "            txt = plt.text(xtext, ytext, str(i), fontsize=24)\n",
        "            txt.set_path_effects([pe.Stroke(linewidth=5, foreground=\"w\"), pe.Normal()])\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.gcf().tight_layout()\n",
        "        plt.savefig('Ours_hash_codes_visulization_{:d}bits.pdf'.format(self.hparams.encode_length), bbox_inches='tight', pad_inches=0.0)\n",
        "\n",
        "    def flag_hparams(self):\n",
        "        flags = '%s' % (self.hparams.model_path)\n",
        "        for hparam in vars(self.hparams):\n",
        "            val = getattr(self.hparams, hparam)\n",
        "            if str(val) == 'False':\n",
        "                continue\n",
        "            elif str(val) == 'True':\n",
        "                flags += ' --%s' % (hparam)\n",
        "            elif str(hparam) in {'model_path', 'num_runs',\n",
        "                                 'num_workers'}:\n",
        "                continue\n",
        "            else:\n",
        "                flags += ' --%s %s' % (hparam, val)\n",
        "        return flags\n",
        "\n",
        "    @staticmethod\n",
        "    def get_general_hparams_grid():\n",
        "        grid = OrderedDict({\n",
        "            'seed': list(range(100000)),\n",
        "            'lr': [0.003, 0.001, 0.0003, 0.0001],\n",
        "            'batch_size': [64, 128, 256],\n",
        "            })\n",
        "        return grid\n",
        "\n",
        "    @staticmethod\n",
        "    def get_general_argparser():\n",
        "        parser = argparse.ArgumentParser()\n",
        "\n",
        "        parser.add_argument('model_path', type=str)\n",
        "        parser.add_argument('--train', action='store_true',\n",
        "                            help='train a model?')\n",
        "        parser.add_argument('-d', '--dataset', default = 'cifar10', type=str,\n",
        "                            help='dataset [%(default)s]')\n",
        "        parser.add_argument(\"-l\",\"--encode_length\", type = int, default=16,\n",
        "                            help = \"Number of bits of the hash code [%(default)d]\")\n",
        "        parser.add_argument(\"--lr\", default = 1e-3, type = float,\n",
        "                            help='initial learning rate [%(default)g]')\n",
        "        parser.add_argument(\"--batch_size\", default=64,type=int,\n",
        "                            help='batch size [%(default)d]')\n",
        "        parser.add_argument(\"-e\",\"--epochs\", default=60, type=int,\n",
        "                            help='max number of epochs [%(default)d]')\n",
        "        parser.add_argument('--cuda', action='store_true',\n",
        "                            help='use CUDA?')\n",
        "        parser.add_argument('--num_runs', type=int, default=1,\n",
        "                            help='num random runs (not random if 1) '\n",
        "                            '[%(default)d]')\n",
        "        parser.add_argument('--num_bad_epochs', type=int, default=6,\n",
        "                            help='num indulged bad epochs [%(default)d]')\n",
        "        parser.add_argument('--validate_frequency', type=int, default=20,\n",
        "                            help='validate every [%(default)d] epochs')\n",
        "        parser.add_argument('--num_workers', type=int, default=8,\n",
        "                            help='num dataloader workers [%(default)d]')\n",
        "        parser.add_argument('--seed', type=int, default=8888,\n",
        "                            help='random seed [%(default)d]')\n",
        "        parser.add_argument('--device', type=int, default=0,\n",
        "                            help='device of the gpu')\n",
        "\n",
        "\n",
        "        return parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFTRpH-4NktS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "\n",
        "# from model.base_model import Base_Model\n",
        "\n",
        "class CIBHash(Base_Model):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__(hparams=hparams)\n",
        "\n",
        "    def define_parameters(self):\n",
        "        self.vgg = torchvision.models.vgg16(pretrained=True)\n",
        "        self.vgg.classifier = nn.Sequential(*list(self.vgg.classifier.children())[:6])\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.encoder = nn.Sequential(nn.Linear(4096, 1024),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Linear(1024, self.hparams.encode_length),\n",
        "                                      )\n",
        "\n",
        "        self.criterion = NtXentLoss(self.hparams.batch_size, self.hparams.temperature)\n",
        "\n",
        "    def forward(self, imgi, imgj, device):\n",
        "        imgi = self.vgg.features(imgi)\n",
        "        imgi = imgi.view(imgi.size(0), -1)\n",
        "        imgi = self.vgg.classifier(imgi)\n",
        "        prob_i = torch.sigmoid(self.encoder(imgi))\n",
        "        z_i = hash_layer(prob_i - 0.5)\n",
        "\n",
        "        imgj = self.vgg.features(imgj)\n",
        "        imgj = imgj.view(imgj.size(0), -1)\n",
        "        imgj = self.vgg.classifier(imgj)\n",
        "        prob_j = torch.sigmoid(self.encoder(imgj))\n",
        "        z_j = hash_layer(prob_j - 0.5)\n",
        "\n",
        "        kl_loss = (self.compute_kl(prob_i, prob_j) + self.compute_kl(prob_j, prob_i)) / 2\n",
        "        contra_loss = self.criterion(z_i, z_j, device)\n",
        "        loss = contra_loss + self.hparams.weight * kl_loss\n",
        "\n",
        "        return {'loss': loss, 'contra_loss': contra_loss, 'kl_loss': kl_loss}\n",
        "\n",
        "    def encode_discrete(self, x):\n",
        "        x = self.vgg.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.vgg.classifier(x)\n",
        "\n",
        "        prob = torch.sigmoid(self.encoder(x))\n",
        "        z = hash_layer(prob - 0.5)\n",
        "\n",
        "        return z\n",
        "\n",
        "    def compute_kl(self, prob, prob_v):\n",
        "        prob_v = prob_v.detach()\n",
        "        # prob = prob.detach()\n",
        "\n",
        "        kl = prob * (torch.log(prob + 1e-8) - torch.log(prob_v + 1e-8)) + (1 - prob) * (torch.log(1 - prob + 1e-8 ) - torch.log(1 - prob_v + 1e-8))\n",
        "        kl = torch.mean(torch.sum(kl, axis = 1))\n",
        "        return kl\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam([{'params': self.encoder.parameters()}], lr = self.hparams.lr)\n",
        "\n",
        "    def get_hparams_grid(self):\n",
        "        grid = Base_Model.get_general_hparams_grid()\n",
        "        grid.update({\n",
        "            'temperature': [0.2, 0.3, 0.4],\n",
        "            'weight': [0.001, 0.005, 0.0005, 0.0001, 0.00005, 0.00001]\n",
        "            })\n",
        "        return grid\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_specific_argparser():\n",
        "        parser = Base_Model.get_general_argparser()\n",
        "\n",
        "        parser.add_argument(\"-t\", \"--temperature\", default = 0.3, type = float,\n",
        "                            help = \"Temperature [%(default)d]\",)\n",
        "        parser.add_argument('-w',\"--weight\", default = 0.001, type=float,\n",
        "                            help='weight of I(x,z) [%(default)f]')\n",
        "        return parser\n",
        "\n",
        "\n",
        "class hash(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        # ctx.save_for_backward(input)\n",
        "        return torch.sign(input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # input,  = ctx.saved_tensors\n",
        "        # grad_output = grad_output.data\n",
        "\n",
        "        return grad_output\n",
        "\n",
        "# class hash(torch.autograd.Function):\n",
        "#     @staticmethod\n",
        "#     def forward(ctx, input):\n",
        "#         ctx.save_for_backward(input)\n",
        "#         return input / (1 + torch.abs(input))\n",
        "\n",
        "#     @staticmethod\n",
        "#     def backward(ctx, grad_output):\n",
        "#         input, = ctx.saved_tensors\n",
        "#         grad_input = grad_output.clone()\n",
        "#         grad_input /= torch.pow(1 + torch.abs(input), 2)\n",
        "#         return grad_input\n",
        "\n",
        "def hash_layer(input):\n",
        "    return hash.apply(input)\n",
        "\n",
        "class NtXentLoss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature):\n",
        "        super(NtXentLoss, self).__init__()\n",
        "        #self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        #self.device = device\n",
        "\n",
        "        #self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.similarityF = nn.CosineSimilarity(dim = 2)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def forward(self, z_i, z_j, device):\n",
        "        \"\"\"\n",
        "        We do not sample negative examples explicitly.\n",
        "        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N − 1) augmented examples within a minibatch as negative examples.\n",
        "        \"\"\"\n",
        "        batch_size = z_i.shape[0]\n",
        "        N = 2 * batch_size\n",
        "\n",
        "        z = torch.cat((z_i, z_j), dim=0)\n",
        "\n",
        "        sim = self.similarityF(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
        "        #sim = 0.5 * (z_i.shape[1] - torch.tensordot(z.unsqueeze(1), z.T.unsqueeze(0), dims = 2)) / z_i.shape[1] / self.temperature\n",
        "\n",
        "        sim_i_j = torch.diag(sim, batch_size )\n",
        "        sim_j_i = torch.diag(sim, -batch_size )\n",
        "\n",
        "        mask = self.mask_correlated_samples(batch_size)\n",
        "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).view(N, 1)\n",
        "        negative_samples = sim[mask].view(N, -1)\n",
        "\n",
        "        labels = torch.zeros(N).to(device).long()\n",
        "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        loss /= N\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "78xauqxkOFk6",
        "outputId": "15194aa3-8a82-4a98-8900-49f44bd63836"
      },
      "outputs": [],
      "source": [
        "args_list = ['nuswide', '--train', '--dataset', 'nuswide', '--encode_length', '64', '--cuda', '--device', '0', '--epochs', '40']\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('model_path', type=str, help='Path to the model or dataset name')\n",
        "    parser.add_argument('--train', action='store_true', help='Train the model')\n",
        "    parser.add_argument('--dataset', type=str, help='Name of the dataset', default='cifar10')\n",
        "    parser.add_argument('--encode_length', type=int, help='Length of encoding', default=16)\n",
        "    parser.add_argument('--cuda', action='store_true', help='Use CUDA for computation')\n",
        "    parser.add_argument('--lr', type=float, help='Learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, help='Batch size')\n",
        "    parser.add_argument('--epochs', type=int, help='Number of epochs')\n",
        "    parser.add_argument('--num_runs', type=int, help='Number of runs')\n",
        "    parser.add_argument('--num_bad_epochs', type=int, help='Number of bad epochs for early stopping')\n",
        "    parser.add_argument('--validate_frequency', type=int, help='Frequency of validation')\n",
        "    parser.add_argument('--num_workers', type=int, help='Number of workers')\n",
        "    parser.add_argument('--seed', type=int, help='Random seed')\n",
        "    parser.add_argument('--device', type=str, help='Device to use', default='0')\n",
        "    parser.add_argument('--temperature', type=float, help='Temperature for sampling')\n",
        "    parser.add_argument('--weight', type=float, help='Weight parameter')\n",
        "\n",
        "    args = parser.parse_args(args_list)\n",
        "\n",
        "    # Now you can use args to access the command-line arguments\n",
        "    print(f\"Model Path: {args.model_path}\")\n",
        "    if args.train:\n",
        "        print(\"Training mode activated\")\n",
        "    print(f\"Dataset: {args.dataset}\")\n",
        "    print(f\"Encode Length: {args.encode_length}\")\n",
        "    if args.cuda:\n",
        "        print(\"Using CUDA\")\n",
        "\n",
        "    # 用相同的args_list重新解析参数\n",
        "    argparser = CIBHash.get_model_specific_argparser()\n",
        "    hparams = argparser.parse_args(args_list)\n",
        "\n",
        "    # 设置CUDA设备\n",
        "    if hparams.cuda:\n",
        "        torch.cuda.set_device(hparams.device)\n",
        "\n",
        "    # 初始化模型\n",
        "    model = CIBHash(hparams)\n",
        "\n",
        "    # 根据参数决定是否训练模型\n",
        "    if hparams.train:\n",
        "        model.run_training_sessions()\n",
        "    else:\n",
        "        model.load()\n",
        "        print('Loaded model with: %s' % model.flag_hparams())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "luyqOy-5rYOU",
        "outputId": "e5ba6471-9217-4a91-85fb-93ff38a58d5f"
      },
      "outputs": [],
      "source": [
        "# 如果需要重新训练模型：\n",
        "# !ls\n",
        "# !rm nuswide\n",
        "# !ls"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
