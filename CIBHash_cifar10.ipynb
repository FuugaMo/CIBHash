{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iCcHw_0IMLWO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "np.random.seed(0)\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZiMDkJ0sMNY4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import statistics as stat\n",
        "import sys\n",
        "\n",
        "class Logger(object):\n",
        "\n",
        "    def __init__(self, log_path, on=True):\n",
        "        self.log_path = log_path\n",
        "        self.on = False\n",
        "\n",
        "        if self.on:\n",
        "            while os.path.isfile(self.log_path):\n",
        "                self.log_path += '+'\n",
        "\n",
        "    def log(self, string, newline=True):\n",
        "        if self.on:\n",
        "            with open(self.log_path, 'a') as logf:\n",
        "                logf.write(string)\n",
        "                if newline: logf.write('\\n')\n",
        "\n",
        "        sys.stdout.write(string)\n",
        "        if newline: sys.stdout.write('\\n')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def log_perfs(self, perfs):\n",
        "        valid_perfs = [perf for perf in perfs if not math.isinf(perf)]\n",
        "        best_perf = max(valid_perfs)\n",
        "        self.log('-' * 89)\n",
        "        self.log('%d perfs: %s' % (len(perfs), str(perfs)))\n",
        "        self.log('perf max: %g' % best_perf)\n",
        "        self.log('perf min: %g' % min(valid_perfs))\n",
        "        self.log('perf avg: %g' % stat.mean(valid_perfs))\n",
        "        self.log('perf std: %g' % (stat.stdev(valid_perfs)\n",
        "                                     if len(valid_perfs) > 1 else 0.0))\n",
        "        self.log('(excluded %d out of %d runs that produced -inf)' %\n",
        "                 (len(perfs) - len(valid_perfs), len(perfs)))\n",
        "        self.log('-' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hnDJd9gDMOlA"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compress(train, test, encode_discrete, device):\n",
        "    retrievalB = list([])\n",
        "    retrievalL = list([])\n",
        "    for batch_step, (data, target) in enumerate(train):\n",
        "        var_data = Variable(data.to(device))\n",
        "        code = encode_discrete(var_data)\n",
        "        retrievalB.extend(code.cpu().data.numpy())\n",
        "        retrievalL.extend(target)\n",
        "\n",
        "    queryB = list([])\n",
        "    queryL = list([])\n",
        "    for batch_step, (data, target) in enumerate(test):\n",
        "        var_data = Variable(data.to(device))\n",
        "        code = encode_discrete(var_data)\n",
        "        queryB.extend(code.cpu().data.numpy())\n",
        "        queryL.extend(target)\n",
        "\n",
        "    retrievalB = np.array(retrievalB)\n",
        "    retrievalL = np.stack(retrievalL)\n",
        "\n",
        "    queryB = np.array(queryB)\n",
        "    queryL = np.stack(queryL)\n",
        "    return retrievalB, retrievalL, queryB, queryL\n",
        "\n",
        "\n",
        "def calculate_hamming(B1, B2):\n",
        "    \"\"\"\n",
        "    :param B1:  vector [n]\n",
        "    :param B2:  vector [r*n]\n",
        "    :return: hamming distance [r]\n",
        "    \"\"\"\n",
        "    q = B2.shape[1] # max inner product value\n",
        "    distH = 0.5 * (q - np.dot(B1, B2.transpose()))\n",
        "    return distH\n",
        "\n",
        "\n",
        "def calculate_top_map(qB, rB, queryL, retrievalL, topk):\n",
        "    \"\"\"\n",
        "    :param qB: {-1,+1}^{mxq} query bits\n",
        "    :param rB: {-1,+1}^{nxq} retrieval bits\n",
        "    :param queryL: {0,1}^{mxl} query label\n",
        "    :param retrievalL: {0,1}^{nxl} retrieval label\n",
        "    :param topk:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_query = queryL.shape[0]\n",
        "    topkmap = 0\n",
        "    for iter in range(num_query):\n",
        "        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n",
        "        hamm = calculate_hamming(qB[iter, :], rB)\n",
        "        ind = np.argsort(hamm)\n",
        "        gnd = gnd[ind] # reorder gnd\n",
        "\n",
        "        tgnd = gnd[0:topk]\n",
        "        tsum = int(np.sum(tgnd))\n",
        "        if tsum == 0:\n",
        "            continue\n",
        "        count = np.linspace(1, tsum, tsum)\n",
        "\n",
        "        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n",
        "        topkmap_ = np.mean(count / (tindex))\n",
        "        # print(topkmap_)\n",
        "        topkmap = topkmap + topkmap_\n",
        "    topkmap = topkmap / num_query\n",
        "    return topkmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gXZTvLGLMQYc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# from utils.gaussian_blur import GaussianBlur\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.load_datasets()\n",
        "\n",
        "        # setup dataTransform\n",
        "        color_jitter = transforms.ColorJitter(0.4,0.4,0.4,0.1)\n",
        "        self.train_transforms = transforms.Compose([transforms.RandomResizedCrop(size = 224,scale=(0.5, 1.0)),\n",
        "                                            transforms.RandomHorizontalFlip(),\n",
        "                                            transforms.RandomApply([color_jitter], p = 0.7),\n",
        "                                            transforms.RandomGrayscale(p  = 0.2),\n",
        "                                            GaussianBlur(3),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                            ])\n",
        "        self.test_transforms = transforms.Compose([\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.test_cifar10_transforms = transforms.Compose([\n",
        "                                            transforms.Resize((224, 224)),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def load_datasets(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_loaders(self, batch_size, num_workers, shuffle_train=False,\n",
        "                    get_test=True):\n",
        "        train_dataset = MyTrainDataset(self.X_train, self.Y_train, self.train_transforms)\n",
        "\n",
        "        if(self.dataset == 'cifar10'):\n",
        "            val_dataset = MyTestDataset(self.X_val, self.Y_val, self.test_cifar10_transforms, self.dataset)\n",
        "            test_dataset = MyTestDataset(self.X_test, self.Y_test, self.test_cifar10_transforms, self.dataset)\n",
        "            database_dataset = MyTestDataset(self.X_database, self.Y_database, self.test_cifar10_transforms, self.dataset)\n",
        "        else:\n",
        "            val_dataset = MyTestDataset(self.X_val, self.Y_val, self.test_transforms, self.dataset)\n",
        "            test_dataset = MyTestDataset(self.X_test, self.Y_test, self.test_transforms, self.dataset)\n",
        "            database_dataset = MyTestDataset(self.X_database, self.Y_database, self.test_transforms, self.dataset)\n",
        "\n",
        "        # DataLoader\n",
        "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
        "                                                shuffle=shuffle_train,\n",
        "                                                num_workers=num_workers)\n",
        "\n",
        "        val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=num_workers)\n",
        "\n",
        "        test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=num_workers) if get_test else None\n",
        "\n",
        "        database_loader = DataLoader(dataset=database_dataset, batch_size=batch_size,\n",
        "                                                    shuffle=False,\n",
        "                                                    num_workers=num_workers)\n",
        "\n",
        "        return train_loader, val_loader, test_loader, database_loader\n",
        "\n",
        "class LabeledData(Data):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__(dataset=dataset)\n",
        "\n",
        "    def load_datasets(self):\n",
        "        if(self.dataset == 'cifar10'):\n",
        "            self.topK = 1000\n",
        "            self.X_train, self.Y_train, self.X_val, self.Y_val, self.X_test, self.Y_test, self.X_database, self.Y_database = get_cifar()\n",
        "        else:\n",
        "            raise NotImplementedError(\"Please use the right dataset!\")\n",
        "\n",
        "class MyTrainDataset(Dataset):\n",
        "    def __init__(self,data,labels, transform):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform  = transform\n",
        "    def __getitem__(self, index):\n",
        "        pilImg = Image.fromarray(self.data[index])\n",
        "        imgi = self.transform(pilImg)\n",
        "        imgj = self.transform(pilImg)\n",
        "        return (imgi, imgj, self.labels[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class MyTestDataset(Dataset):\n",
        "    def __init__(self,data,labels, transform,dataset):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform  = transform\n",
        "        self.dataset = dataset\n",
        "    def __getitem__(self, index):\n",
        "        if self.dataset == 'cifar10':\n",
        "            pilImg = Image.fromarray(self.data[index])\n",
        "            return (self.transform(pilImg),self.labels[index])\n",
        "        else:\n",
        "            return (self.transform(self.data[index]),self.labels[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def get_cifar():\n",
        "    # Dataset\n",
        "    train_dataset = dsets.CIFAR10(root='./data/cifar10/',\n",
        "                                train=True,\n",
        "                                download=True)\n",
        "\n",
        "    test_dataset = dsets.CIFAR10(root='./data/cifar10/',\n",
        "                                train=False\n",
        "                                )\n",
        "\n",
        "    database_dataset = dsets.CIFAR10(root='./data/cifar10/',\n",
        "                                    train=True\n",
        "                                    )\n",
        "\n",
        "\n",
        "    # train with 5000 images\n",
        "    X = train_dataset.data\n",
        "    L = np.array(train_dataset.targets)\n",
        "\n",
        "    first = True\n",
        "    for label in range(10):\n",
        "        index = np.where(L == label)[0]\n",
        "        N = index.shape[0]\n",
        "        prem = np.random.permutation(N)\n",
        "        index = index[prem]\n",
        "\n",
        "        data = X[index[0:500]]\n",
        "        labels = L[index[0: 500]]\n",
        "        if first:\n",
        "            Y_train = labels\n",
        "            X_train = data\n",
        "        else:\n",
        "            Y_train = np.concatenate((Y_train, labels))\n",
        "            X_train = np.concatenate((X_train, data))\n",
        "        first = False\n",
        "\n",
        "    Y_train = np.eye(10)[Y_train]\n",
        "\n",
        "\n",
        "    idxs = list(range(len(test_dataset.data)))\n",
        "    np.random.shuffle(idxs)\n",
        "    test_data = np.array(test_dataset.data)\n",
        "    test_tragets = np.array(test_dataset.targets)\n",
        "\n",
        "    X_val = test_data[idxs[:5000]]\n",
        "    Y_val = np.eye(10)[test_tragets[idxs[:5000]]]\n",
        "\n",
        "    X_test = test_data[idxs[5000:]]\n",
        "    Y_test = np.eye(10)[test_tragets[idxs[5000:]]]\n",
        "\n",
        "\n",
        "    X_database = database_dataset.data\n",
        "    Y_database = np.eye(10)[database_dataset.targets]\n",
        "\n",
        "    return X_train, Y_train, X_val, Y_val, X_test, Y_test, X_database, Y_database\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U5J2WOhoIC_w"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import sklearn\n",
        "import argparse\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from datetime import timedelta\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.patheffects as pe\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "from sklearn.datasets import load_digits\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "# from utils.logger import Logger\n",
        "# from utils.data import LabeledData\n",
        "# from utils.evaluation import calculate_hamming\n",
        "# from utils.evaluation import compress, calculate_top_map\n",
        "\n",
        "class Base_Model(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        self.data = LabeledData(self.hparams.dataset)\n",
        "\n",
        "    def get_hparams_grid(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def define_parameters(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run_training_sessions(self):\n",
        "        logger = Logger(self.hparams.model_path + '.log', on=True)\n",
        "        val_perfs = []\n",
        "        best_val_perf = float('-inf')\n",
        "        start = timer()\n",
        "        random.seed(self.hparams.seed)  # For reproducible random runs\n",
        "\n",
        "        for run_num in range(1, self.hparams.num_runs + 1):\n",
        "            state_dict, val_perf = self.run_training_session(run_num, logger)\n",
        "            val_perfs.append(val_perf)\n",
        "\n",
        "            if val_perf > best_val_perf:\n",
        "                best_val_perf = val_perf\n",
        "                logger.log('----New best {:8.4f}, saving'.format(val_perf))\n",
        "                torch.save({'hparams': self.hparams,\n",
        "                            'state_dict': state_dict}, self.hparams.model_path)\n",
        "\n",
        "        logger.log('Time: %s' % str(timedelta(seconds=round(timer() - start))))\n",
        "        self.load()\n",
        "        if self.hparams.num_runs > 1:\n",
        "            logger.log_perfs(val_perfs)\n",
        "            logger.log('best hparams: ' + self.flag_hparams())\n",
        "\n",
        "        val_perf, test_perf = self.run_test()\n",
        "        logger.log('Val:  {:8.4f}'.format(val_perf))\n",
        "        logger.log('Test: {:8.4f}'.format(test_perf))\n",
        "\n",
        "    def run_training_session(self, run_num, logger):\n",
        "        self.train()\n",
        "\n",
        "        # Scramble hyperparameters if number of runs is greater than 1.\n",
        "        if self.hparams.num_runs > 1:\n",
        "            logger.log('RANDOM RUN: %d/%d' % (run_num, self.hparams.num_runs))\n",
        "            for hparam, values in self.get_hparams_grid().items():\n",
        "                assert hasattr(self.hparams, hparam)\n",
        "                self.hparams.__dict__[hparam] = random.choice(values)\n",
        "\n",
        "        random.seed(self.hparams.seed)\n",
        "        torch.manual_seed(self.hparams.seed)\n",
        "\n",
        "        self.define_parameters()\n",
        "\n",
        "        # if encode_length is 16, then al least 80 epochs!\n",
        "        if self.hparams.encode_length == 16:\n",
        "            self.hparams.epochs = max(80, self.hparams.epochs)\n",
        "\n",
        "        logger.log('hparams: %s' % self.flag_hparams())\n",
        "\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        self.to(device)\n",
        "\n",
        "        optimizer = self.configure_optimizers()\n",
        "        train_loader, val_loader, _, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=True, get_test=False)\n",
        "        best_val_perf = float('-inf')\n",
        "        best_state_dict = None\n",
        "        bad_epochs = 0\n",
        "\n",
        "        try:\n",
        "            for epoch in range(1, self.hparams.epochs + 1):\n",
        "                forward_sum = {}\n",
        "                num_steps = 0\n",
        "                for batch_num, batch in enumerate(train_loader):\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    imgi, imgj, _ = batch\n",
        "                    imgi = imgi.to(device)\n",
        "                    imgj = imgj.to(device)\n",
        "\n",
        "                    forward = self.forward(imgi, imgj, device)\n",
        "\n",
        "                    for key in forward:\n",
        "                        if key in forward_sum:\n",
        "                            forward_sum[key] += forward[key]\n",
        "                        else:\n",
        "                            forward_sum[key] = forward[key]\n",
        "                    num_steps += 1\n",
        "\n",
        "                    if math.isnan(forward_sum['loss']):\n",
        "                        logger.log('Stopping epoch because loss is NaN')\n",
        "                        break\n",
        "\n",
        "                    forward['loss'].backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                if math.isnan(forward_sum['loss']):\n",
        "                    logger.log('Stopping training session because loss is NaN')\n",
        "                    break\n",
        "\n",
        "                logger.log('End of epoch {:3d}'.format(epoch), False)\n",
        "                logger.log(' '.join([' | {:s} {:8.4f}'.format(\n",
        "                    key, forward_sum[key] / num_steps)\n",
        "                                     for key in forward_sum]), True)\n",
        "\n",
        "                if epoch % self.hparams.validate_frequency == 0:\n",
        "                    print('evaluating...')\n",
        "                    val_perf = self.evaluate(database_loader, val_loader, self.data.topK, device)\n",
        "                    logger.log(' | val perf {:8.4f}'.format(val_perf), False)\n",
        "\n",
        "                    if val_perf > best_val_perf:\n",
        "                        best_val_perf = val_perf\n",
        "                        bad_epochs = 0\n",
        "                        logger.log('\\t\\t*Best model so far, deep copying*')\n",
        "                        best_state_dict = deepcopy(self.state_dict())\n",
        "                    else:\n",
        "                        bad_epochs += 1\n",
        "                        logger.log('\\t\\tBad epoch %d' % bad_epochs)\n",
        "\n",
        "                    if bad_epochs > self.hparams.num_bad_epochs:\n",
        "                        break\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.log('-' * 89)\n",
        "            logger.log('Exiting from training early')\n",
        "\n",
        "        return best_state_dict, best_val_perf\n",
        "\n",
        "    def evaluate(self, database_loader, val_loader, topK, device):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            retrievalB, retrievalL, queryB, queryL = compress(database_loader, val_loader, self.encode_discrete, device)\n",
        "            result = calculate_top_map(qB=queryB, rB=retrievalB, queryL=queryL, retrievalL=retrievalL, topk=topK)\n",
        "        self.train()\n",
        "        return result\n",
        "\n",
        "    def load(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        checkpoint = torch.load(self.hparams.model_path) if self.hparams.cuda \\\n",
        "                     else torch.load(self.hparams.model_path,\n",
        "                                     map_location=torch.device('cpu'))\n",
        "        if checkpoint['hparams'].cuda and not self.hparams.cuda:\n",
        "            checkpoint['hparams'].cuda = False\n",
        "        self.hparams = checkpoint['hparams']\n",
        "        self.define_parameters()\n",
        "        self.load_state_dict(checkpoint['state_dict'])\n",
        "        self.to(device)\n",
        "\n",
        "    def run_test(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        _, val_loader, test_loader, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        val_perf = self.evaluate(database_loader, val_loader, self.data.topK, device)\n",
        "        test_perf = self.evaluate(database_loader, test_loader, self.data.topK, device)\n",
        "        return val_perf, test_perf\n",
        "\n",
        "    def run_retrieval_case_study(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        query_idxs = [0,2,5]\n",
        "        X_database = self.data.X_database\n",
        "        X_test = self.data.X_test\n",
        "        X_case = torch.cat([self.data.test_cifar10_transforms(Image.fromarray(self.data.X_test[i])).unsqueeze(0) for i in query_idxs], dim=0)\n",
        "        _, val_loader, test_loader, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        # get hash codes\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            retrievalB = list([])\n",
        "            for batch_step, (data, target) in enumerate(database_loader):\n",
        "                var_data = Variable(data.to(device))\n",
        "                code = self.encode_discrete(var_data)\n",
        "                retrievalB.extend(code.cpu().data.numpy())\n",
        "\n",
        "            queryB = list([])\n",
        "            var_data = Variable(X_case.to(device))\n",
        "            code = self.encode_discrete(var_data)\n",
        "            queryB.extend(code.cpu().data.numpy())\n",
        "\n",
        "        retrievalB = np.array(retrievalB)\n",
        "        queryB = np.array(queryB)\n",
        "\n",
        "        # get top 10 index\n",
        "        top10_idx_list = []\n",
        "        for idx in range(queryB.shape[0]):\n",
        "            hamm = calculate_hamming(queryB[idx, :], retrievalB)\n",
        "            ind = list(np.argsort(hamm)[:10])\n",
        "            top10_idx_list.append(ind)\n",
        "\n",
        "        # plot results\n",
        "        fig = plt.figure(0, figsize = (5,1.2))\n",
        "        fig.clf()\n",
        "        gs = gridspec.GridSpec(queryB.shape[0], 12)\n",
        "        gs.update(wspace = 0.0001, hspace = 0.0001)\n",
        "        for i in range(queryB.shape[0]):\n",
        "            axes = plt.subplot(gs[i,0])\n",
        "            axes.imshow(X_test[query_idxs[i]])\n",
        "            axes.axis('off')\n",
        "\n",
        "            for j in range(0, 10):\n",
        "                axes = plt.subplot(gs[i, j+2])\n",
        "                axes.imshow(X_database[top10_idx_list[i][j]])\n",
        "                axes.axis('off')\n",
        "        fig.savefig(\"retrieval_case_study_{:d}bits.pdf\".format(self.hparams.encode_length), bbox_inches='tight', pad_inches=0.0)\n",
        "\n",
        "    def hash_code_visualization(self):\n",
        "        \"\"\"\n",
        "        cifar10 labels:\n",
        "        0: Airplane 1: Automobile 2: Bird 3: Cat 4: Deer\n",
        "        5: Dog 6: Frog 7: Horse 8: Ship 9: Truck\n",
        "        \"\"\"\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        _, _, test_loader, _ = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        retrievalB = list([])\n",
        "        retrievalL = list([])\n",
        "        for batch_step, (data, target) in enumerate(test_loader):\n",
        "            var_data = Variable(data.to(device))\n",
        "            code = self.encode_discrete(var_data)\n",
        "            retrievalB.extend(code.cpu().data.numpy())\n",
        "            retrievalL.extend(target.cpu().data.numpy())\n",
        "\n",
        "        hash_codes = np.array(retrievalB)\n",
        "        _, labels = np.where(np.array(retrievalL) == 1)\n",
        "        labels_ticks = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "\n",
        "        # TSN\n",
        "        mapper = TSNE(perplexity=30).fit_transform(hash_codes)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(mapper[:,0], mapper[:,1], lw=0, s=20, c=labels.astype(np.int), cmap='Spectral')\n",
        "        # cbar = plt.colorbar(boundaries=np.arange(11)-0.5, fraction=0.046, pad=0.04)\n",
        "        # cbar.set_ticks(np.arange(10))\n",
        "        # cbar.set_ticklabels(labels_ticks)\n",
        "\n",
        "        # Add the labels for each digit.\n",
        "        for i in range(10):\n",
        "            # Position of each label.\n",
        "            xtext, ytext = np.median(mapper[labels == i, :], axis=0)\n",
        "            txt = plt.text(xtext, ytext, str(i), fontsize=24)\n",
        "            txt.set_path_effects([pe.Stroke(linewidth=5, foreground=\"w\"), pe.Normal()])\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.gcf().tight_layout()\n",
        "        plt.savefig('Ours_hash_codes_visulization_{:d}bits.pdf'.format(self.hparams.encode_length), bbox_inches='tight', pad_inches=0.0)\n",
        "\n",
        "    def flag_hparams(self):\n",
        "        flags = '%s' % (self.hparams.model_path)\n",
        "        for hparam in vars(self.hparams):\n",
        "            val = getattr(self.hparams, hparam)\n",
        "            if str(val) == 'False':\n",
        "                continue\n",
        "            elif str(val) == 'True':\n",
        "                flags += ' --%s' % (hparam)\n",
        "            elif str(hparam) in {'model_path', 'num_runs',\n",
        "                                 'num_workers'}:\n",
        "                continue\n",
        "            else:\n",
        "                flags += ' --%s %s' % (hparam, val)\n",
        "        return flags\n",
        "\n",
        "    @staticmethod\n",
        "    def get_general_hparams_grid():\n",
        "        grid = OrderedDict({\n",
        "            'seed': list(range(100000)),\n",
        "            'lr': [0.003, 0.001, 0.0003, 0.0001],\n",
        "            'batch_size': [64, 128, 256],\n",
        "            })\n",
        "        return grid\n",
        "\n",
        "    @staticmethod\n",
        "    def get_general_argparser():\n",
        "        parser = argparse.ArgumentParser()\n",
        "\n",
        "        parser.add_argument('model_path', type=str)\n",
        "        parser.add_argument('--train', action='store_true',\n",
        "                            help='train a model?')\n",
        "        parser.add_argument('-d', '--dataset', default = 'cifar10', type=str,\n",
        "                            help='dataset [%(default)s]')\n",
        "        parser.add_argument(\"-l\",\"--encode_length\", type = int, default=16,\n",
        "                            help = \"Number of bits of the hash code [%(default)d]\")\n",
        "        parser.add_argument(\"--lr\", default = 1e-3, type = float,\n",
        "                            help='initial learning rate [%(default)g]')\n",
        "        parser.add_argument(\"--batch_size\", default=64,type=int,\n",
        "                            help='batch size [%(default)d]')\n",
        "        parser.add_argument(\"-e\",\"--epochs\", default=60, type=int,\n",
        "                            help='max number of epochs [%(default)d]')\n",
        "        parser.add_argument('--cuda', action='store_true',\n",
        "                            help='use CUDA?')\n",
        "        parser.add_argument('--num_runs', type=int, default=1,\n",
        "                            help='num random runs (not random if 1) '\n",
        "                            '[%(default)d]')\n",
        "        parser.add_argument('--num_bad_epochs', type=int, default=6,\n",
        "                            help='num indulged bad epochs [%(default)d]')\n",
        "        parser.add_argument('--validate_frequency', type=int, default=20,\n",
        "                            help='validate every [%(default)d] epochs')\n",
        "        parser.add_argument('--num_workers', type=int, default=8,\n",
        "                            help='num dataloader workers [%(default)d]')\n",
        "        parser.add_argument('--seed', type=int, default=8888,\n",
        "                            help='random seed [%(default)d]')\n",
        "        parser.add_argument('--device', type=int, default=0,\n",
        "                            help='device of the gpu')\n",
        "\n",
        "\n",
        "        return parser\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vylyrlM2M0Y_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import sklearn\n",
        "import argparse\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from datetime import timedelta\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.patheffects as pe\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "from sklearn.datasets import load_digits\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "# from utils.logger import Logger\n",
        "# from utils.data import LabeledData\n",
        "# from utils.evaluation import calculate_hamming\n",
        "# from utils.evaluation import compress, calculate_top_map\n",
        "\n",
        "class Base_Model(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        self.data = LabeledData(self.hparams.dataset)\n",
        "\n",
        "    def get_hparams_grid(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def define_parameters(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run_training_sessions(self):\n",
        "        logger = Logger(self.hparams.model_path + '.log', on=True)\n",
        "        val_perfs = []\n",
        "        best_val_perf = float('-inf')\n",
        "        start = timer()\n",
        "        random.seed(self.hparams.seed)  # For reproducible random runs\n",
        "\n",
        "        for run_num in range(1, self.hparams.num_runs + 1):\n",
        "            state_dict, val_perf = self.run_training_session(run_num, logger)\n",
        "            val_perfs.append(val_perf)\n",
        "\n",
        "            if val_perf > best_val_perf:\n",
        "                best_val_perf = val_perf\n",
        "                logger.log('----New best {:8.4f}, saving'.format(val_perf))\n",
        "                torch.save({'hparams': self.hparams,\n",
        "                            'state_dict': state_dict}, self.hparams.model_path)\n",
        "\n",
        "        logger.log('Time: %s' % str(timedelta(seconds=round(timer() - start))))\n",
        "        self.load()\n",
        "        if self.hparams.num_runs > 1:\n",
        "            logger.log_perfs(val_perfs)\n",
        "            logger.log('best hparams: ' + self.flag_hparams())\n",
        "\n",
        "        val_perf, test_perf = self.run_test()\n",
        "        logger.log('Val:  {:8.4f}'.format(val_perf))\n",
        "        logger.log('Test: {:8.4f}'.format(test_perf))\n",
        "\n",
        "    def run_training_session(self, run_num, logger):\n",
        "        self.train()\n",
        "\n",
        "        # Scramble hyperparameters if number of runs is greater than 1.\n",
        "        if self.hparams.num_runs > 1:\n",
        "            logger.log('RANDOM RUN: %d/%d' % (run_num, self.hparams.num_runs))\n",
        "            for hparam, values in self.get_hparams_grid().items():\n",
        "                assert hasattr(self.hparams, hparam)\n",
        "                self.hparams.__dict__[hparam] = random.choice(values)\n",
        "\n",
        "        random.seed(self.hparams.seed)\n",
        "        torch.manual_seed(self.hparams.seed)\n",
        "\n",
        "        self.define_parameters()\n",
        "\n",
        "        # if encode_length is 16, then al least 80 epochs!\n",
        "        if self.hparams.encode_length == 16:\n",
        "            self.hparams.epochs = max(80, self.hparams.epochs)\n",
        "\n",
        "        logger.log('hparams: %s' % self.flag_hparams())\n",
        "\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        self.to(device)\n",
        "\n",
        "        optimizer = self.configure_optimizers()\n",
        "        train_loader, val_loader, _, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=True, get_test=False)\n",
        "        best_val_perf = float('-inf')\n",
        "        best_state_dict = None\n",
        "        bad_epochs = 0\n",
        "\n",
        "        try:\n",
        "            for epoch in range(1, self.hparams.epochs + 1):\n",
        "                forward_sum = {}\n",
        "                num_steps = 0\n",
        "                for batch_num, batch in enumerate(train_loader):\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    imgi, imgj, _ = batch\n",
        "                    imgi = imgi.to(device)\n",
        "                    imgj = imgj.to(device)\n",
        "\n",
        "                    forward = self.forward(imgi, imgj, device)\n",
        "\n",
        "                    for key in forward:\n",
        "                        if key in forward_sum:\n",
        "                            forward_sum[key] += forward[key]\n",
        "                        else:\n",
        "                            forward_sum[key] = forward[key]\n",
        "                    num_steps += 1\n",
        "\n",
        "                    if math.isnan(forward_sum['loss']):\n",
        "                        logger.log('Stopping epoch because loss is NaN')\n",
        "                        break\n",
        "\n",
        "                    forward['loss'].backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                if math.isnan(forward_sum['loss']):\n",
        "                    logger.log('Stopping training session because loss is NaN')\n",
        "                    break\n",
        "\n",
        "                logger.log('End of epoch {:3d}'.format(epoch), False)\n",
        "                logger.log(' '.join([' | {:s} {:8.4f}'.format(\n",
        "                    key, forward_sum[key] / num_steps)\n",
        "                                     for key in forward_sum]), True)\n",
        "\n",
        "                if epoch % self.hparams.validate_frequency == 0:\n",
        "                    print('evaluating...')\n",
        "                    val_perf = self.evaluate(database_loader, val_loader, self.data.topK, device)\n",
        "                    logger.log(' | val perf {:8.4f}'.format(val_perf), False)\n",
        "\n",
        "                    if val_perf > best_val_perf:\n",
        "                        best_val_perf = val_perf\n",
        "                        bad_epochs = 0\n",
        "                        logger.log('\\t\\t*Best model so far, deep copying*')\n",
        "                        best_state_dict = deepcopy(self.state_dict())\n",
        "                    else:\n",
        "                        bad_epochs += 1\n",
        "                        logger.log('\\t\\tBad epoch %d' % bad_epochs)\n",
        "\n",
        "                    if bad_epochs > self.hparams.num_bad_epochs:\n",
        "                        break\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.log('-' * 89)\n",
        "            logger.log('Exiting from training early')\n",
        "\n",
        "        return best_state_dict, best_val_perf\n",
        "\n",
        "    def evaluate(self, database_loader, val_loader, topK, device):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            retrievalB, retrievalL, queryB, queryL = compress(database_loader, val_loader, self.encode_discrete, device)\n",
        "            result = calculate_top_map(qB=queryB, rB=retrievalB, queryL=queryL, retrievalL=retrievalL, topk=topK)\n",
        "        self.train()\n",
        "        return result\n",
        "\n",
        "    def load(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        checkpoint = torch.load(self.hparams.model_path) if self.hparams.cuda \\\n",
        "                     else torch.load(self.hparams.model_path,\n",
        "                                     map_location=torch.device('cpu'))\n",
        "        if checkpoint['hparams'].cuda and not self.hparams.cuda:\n",
        "            checkpoint['hparams'].cuda = False\n",
        "        self.hparams = checkpoint['hparams']\n",
        "        self.define_parameters()\n",
        "        self.load_state_dict(checkpoint['state_dict'])\n",
        "        self.to(device)\n",
        "\n",
        "    def run_test(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        _, val_loader, test_loader, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        val_perf = self.evaluate(database_loader, val_loader, self.data.topK, device)\n",
        "        test_perf = self.evaluate(database_loader, test_loader, self.data.topK, device)\n",
        "        return val_perf, test_perf\n",
        "\n",
        "    def run_retrieval_case_study(self):\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        query_idxs = [0,2,5]\n",
        "        X_database = self.data.X_database\n",
        "        X_test = self.data.X_test\n",
        "        X_case = torch.cat([self.data.test_cifar10_transforms(Image.fromarray(self.data.X_test[i])).unsqueeze(0) for i in query_idxs], dim=0)\n",
        "        _, val_loader, test_loader, database_loader = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        # get hash codes\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            retrievalB = list([])\n",
        "            for batch_step, (data, target) in enumerate(database_loader):\n",
        "                var_data = Variable(data.to(device))\n",
        "                code = self.encode_discrete(var_data)\n",
        "                retrievalB.extend(code.cpu().data.numpy())\n",
        "\n",
        "            queryB = list([])\n",
        "            var_data = Variable(X_case.to(device))\n",
        "            code = self.encode_discrete(var_data)\n",
        "            queryB.extend(code.cpu().data.numpy())\n",
        "\n",
        "        retrievalB = np.array(retrievalB)\n",
        "        queryB = np.array(queryB)\n",
        "\n",
        "        # get top 10 index\n",
        "        top10_idx_list = []\n",
        "        for idx in range(queryB.shape[0]):\n",
        "            hamm = calculate_hamming(queryB[idx, :], retrievalB)\n",
        "            ind = list(np.argsort(hamm)[:10])\n",
        "            top10_idx_list.append(ind)\n",
        "\n",
        "        # plot results\n",
        "        fig = plt.figure(0, figsize = (5,1.2))\n",
        "        fig.clf()\n",
        "        gs = gridspec.GridSpec(queryB.shape[0], 12)\n",
        "        gs.update(wspace = 0.0001, hspace = 0.0001)\n",
        "        for i in range(queryB.shape[0]):\n",
        "            axes = plt.subplot(gs[i,0])\n",
        "            axes.imshow(X_test[query_idxs[i]])\n",
        "            axes.axis('off')\n",
        "\n",
        "            for j in range(0, 10):\n",
        "                axes = plt.subplot(gs[i, j+2])\n",
        "                axes.imshow(X_database[top10_idx_list[i][j]])\n",
        "                axes.axis('off')\n",
        "        fig.savefig(\"retrieval_case_study_{:d}bits.pdf\".format(self.hparams.encode_length), bbox_inches='tight', pad_inches=0.0)\n",
        "\n",
        "    def hash_code_visualization(self):\n",
        "        \"\"\"\n",
        "        cifar10 labels:\n",
        "        0: Airplane 1: Automobile 2: Bird 3: Cat 4: Deer\n",
        "        5: Dog 6: Frog 7: Horse 8: Ship 9: Truck\n",
        "        \"\"\"\n",
        "        device = torch.device('cuda' if self.hparams.cuda else 'cpu')\n",
        "        _, _, test_loader, _ = self.data.get_loaders(\n",
        "            self.hparams.batch_size, self.hparams.num_workers,\n",
        "            shuffle_train=False, get_test=True)\n",
        "\n",
        "        retrievalB = list([])\n",
        "        retrievalL = list([])\n",
        "        for batch_step, (data, target) in enumerate(test_loader):\n",
        "            var_data = Variable(data.to(device))\n",
        "            code = self.encode_discrete(var_data)\n",
        "            retrievalB.extend(code.cpu().data.numpy())\n",
        "            retrievalL.extend(target.cpu().data.numpy())\n",
        "\n",
        "        hash_codes = np.array(retrievalB)\n",
        "        _, labels = np.where(np.array(retrievalL) == 1)\n",
        "        labels_ticks = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "\n",
        "        # TSN\n",
        "        mapper = TSNE(perplexity=30).fit_transform(hash_codes)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(mapper[:,0], mapper[:,1], lw=0, s=20, c=labels.astype(np.int), cmap='Spectral')\n",
        "        # cbar = plt.colorbar(boundaries=np.arange(11)-0.5, fraction=0.046, pad=0.04)\n",
        "        # cbar.set_ticks(np.arange(10))\n",
        "        # cbar.set_ticklabels(labels_ticks)\n",
        "\n",
        "        # Add the labels for each digit.\n",
        "        for i in range(10):\n",
        "            # Position of each label.\n",
        "            xtext, ytext = np.median(mapper[labels == i, :], axis=0)\n",
        "            txt = plt.text(xtext, ytext, str(i), fontsize=24)\n",
        "            txt.set_path_effects([pe.Stroke(linewidth=5, foreground=\"w\"), pe.Normal()])\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        plt.gcf().tight_layout()\n",
        "        plt.savefig('Ours_hash_codes_visulization_{:d}bits.pdf'.format(self.hparams.encode_length), bbox_inches='tight', pad_inches=0.0)\n",
        "\n",
        "    def flag_hparams(self):\n",
        "        flags = '%s' % (self.hparams.model_path)\n",
        "        for hparam in vars(self.hparams):\n",
        "            val = getattr(self.hparams, hparam)\n",
        "            if str(val) == 'False':\n",
        "                continue\n",
        "            elif str(val) == 'True':\n",
        "                flags += ' --%s' % (hparam)\n",
        "            elif str(hparam) in {'model_path', 'num_runs',\n",
        "                                 'num_workers'}:\n",
        "                continue\n",
        "            else:\n",
        "                flags += ' --%s %s' % (hparam, val)\n",
        "        return flags\n",
        "\n",
        "    @staticmethod\n",
        "    def get_general_hparams_grid():\n",
        "        grid = OrderedDict({\n",
        "            'seed': list(range(100000)),\n",
        "            'lr': [0.003, 0.001, 0.0003, 0.0001],\n",
        "            'batch_size': [64, 128, 256],\n",
        "            })\n",
        "        return grid\n",
        "\n",
        "    @staticmethod\n",
        "    def get_general_argparser():\n",
        "        parser = argparse.ArgumentParser()\n",
        "\n",
        "        parser.add_argument('model_path', type=str)\n",
        "        parser.add_argument('--train', action='store_true',\n",
        "                            help='train a model?')\n",
        "        parser.add_argument('-d', '--dataset', default = 'cifar10', type=str,\n",
        "                            help='dataset [%(default)s]')\n",
        "        parser.add_argument(\"-l\",\"--encode_length\", type = int, default=16,\n",
        "                            help = \"Number of bits of the hash code [%(default)d]\")\n",
        "        parser.add_argument(\"--lr\", default = 1e-3, type = float,\n",
        "                            help='initial learning rate [%(default)g]')\n",
        "        parser.add_argument(\"--batch_size\", default=64,type=int,\n",
        "                            help='batch size [%(default)d]')\n",
        "        parser.add_argument(\"-e\",\"--epochs\", default=60, type=int,\n",
        "                            help='max number of epochs [%(default)d]')\n",
        "        parser.add_argument('--cuda', action='store_true',\n",
        "                            help='use CUDA?')\n",
        "        parser.add_argument('--num_runs', type=int, default=1,\n",
        "                            help='num random runs (not random if 1) '\n",
        "                            '[%(default)d]')\n",
        "        parser.add_argument('--num_bad_epochs', type=int, default=6,\n",
        "                            help='num indulged bad epochs [%(default)d]')\n",
        "        parser.add_argument('--validate_frequency', type=int, default=20,\n",
        "                            help='validate every [%(default)d] epochs')\n",
        "        parser.add_argument('--num_workers', type=int, default=8,\n",
        "                            help='num dataloader workers [%(default)d]')\n",
        "        parser.add_argument('--seed', type=int, default=8888,\n",
        "                            help='random seed [%(default)d]')\n",
        "        parser.add_argument('--device', type=int, default=0,\n",
        "                            help='device of the gpu')\n",
        "\n",
        "\n",
        "        return parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XFTRpH-4NktS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "\n",
        "# from model.base_model import Base_Model\n",
        "\n",
        "class CIBHash(Base_Model):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__(hparams=hparams)\n",
        "\n",
        "    def define_parameters(self):\n",
        "        self.vgg = torchvision.models.vgg16(pretrained=True)\n",
        "        self.vgg.classifier = nn.Sequential(*list(self.vgg.classifier.children())[:6])\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.encoder = nn.Sequential(nn.Linear(4096, 1024),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Linear(1024, self.hparams.encode_length),\n",
        "                                      )\n",
        "\n",
        "        self.criterion = NtXentLoss(self.hparams.batch_size, self.hparams.temperature)\n",
        "\n",
        "    def forward(self, imgi, imgj, device):\n",
        "        imgi = self.vgg.features(imgi)\n",
        "        imgi = imgi.view(imgi.size(0), -1)\n",
        "        imgi = self.vgg.classifier(imgi)\n",
        "        prob_i = torch.sigmoid(self.encoder(imgi))\n",
        "        z_i = hash_layer(prob_i - 0.5)\n",
        "\n",
        "        imgj = self.vgg.features(imgj)\n",
        "        imgj = imgj.view(imgj.size(0), -1)\n",
        "        imgj = self.vgg.classifier(imgj)\n",
        "        prob_j = torch.sigmoid(self.encoder(imgj))\n",
        "        z_j = hash_layer(prob_j - 0.5)\n",
        "\n",
        "        kl_loss = (self.compute_kl(prob_i, prob_j) + self.compute_kl(prob_j, prob_i)) / 2\n",
        "        contra_loss = self.criterion(z_i, z_j, device)\n",
        "        loss = contra_loss + self.hparams.weight * kl_loss\n",
        "\n",
        "        return {'loss': loss, 'contra_loss': contra_loss, 'kl_loss': kl_loss}\n",
        "\n",
        "    def encode_discrete(self, x):\n",
        "        x = self.vgg.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.vgg.classifier(x)\n",
        "\n",
        "        prob = torch.sigmoid(self.encoder(x))\n",
        "        z = hash_layer(prob - 0.5)\n",
        "\n",
        "        return z\n",
        "\n",
        "    def compute_kl(self, prob, prob_v):\n",
        "        prob_v = prob_v.detach()\n",
        "        # prob = prob.detach()\n",
        "\n",
        "        kl = prob * (torch.log(prob + 1e-8) - torch.log(prob_v + 1e-8)) + (1 - prob) * (torch.log(1 - prob + 1e-8 ) - torch.log(1 - prob_v + 1e-8))\n",
        "        kl = torch.mean(torch.sum(kl, axis = 1))\n",
        "        return kl\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam([{'params': self.encoder.parameters()}], lr = self.hparams.lr)\n",
        "\n",
        "    def get_hparams_grid(self):\n",
        "        grid = Base_Model.get_general_hparams_grid()\n",
        "        grid.update({\n",
        "            'temperature': [0.2, 0.3, 0.4],\n",
        "            'weight': [0.001, 0.005, 0.0005, 0.0001, 0.00005, 0.00001]\n",
        "            })\n",
        "        return grid\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_specific_argparser():\n",
        "        parser = Base_Model.get_general_argparser()\n",
        "\n",
        "        parser.add_argument(\"-t\", \"--temperature\", default = 0.3, type = float,\n",
        "                            help = \"Temperature [%(default)d]\",)\n",
        "        parser.add_argument('-w',\"--weight\", default = 0.001, type=float,\n",
        "                            help='weight of I(x,z) [%(default)f]')\n",
        "        return parser\n",
        "\n",
        "\n",
        "class hash(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        # ctx.save_for_backward(input)\n",
        "        return torch.sign(input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # input,  = ctx.saved_tensors\n",
        "        # grad_output = grad_output.data\n",
        "\n",
        "        return grad_output\n",
        "\n",
        "def hash_layer(input):\n",
        "    return hash.apply(input)\n",
        "\n",
        "class NtXentLoss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature):\n",
        "        super(NtXentLoss, self).__init__()\n",
        "        #self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        #self.device = device\n",
        "\n",
        "        #self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.similarityF = nn.CosineSimilarity(dim = 2)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def forward(self, z_i, z_j, device):\n",
        "        \"\"\"\n",
        "        We do not sample negative examples explicitly.\n",
        "        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N  1) augmented examples within a minibatch as negative examples.\n",
        "        \"\"\"\n",
        "        batch_size = z_i.shape[0]\n",
        "        N = 2 * batch_size\n",
        "\n",
        "        z = torch.cat((z_i, z_j), dim=0)\n",
        "\n",
        "        sim = self.similarityF(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
        "        #sim = 0.5 * (z_i.shape[1] - torch.tensordot(z.unsqueeze(1), z.T.unsqueeze(0), dims = 2)) / z_i.shape[1] / self.temperature\n",
        "\n",
        "        sim_i_j = torch.diag(sim, batch_size )\n",
        "        sim_j_i = torch.diag(sim, -batch_size )\n",
        "\n",
        "        mask = self.mask_correlated_samples(batch_size)\n",
        "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).view(N, 1)\n",
        "        negative_samples = sim[mask].view(N, -1)\n",
        "\n",
        "        labels = torch.zeros(N).to(device).long()\n",
        "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        loss /= N\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78xauqxkOFk6",
        "outputId": "2f43960b-b4ba-48f0-dac0-bb82690682e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Path: cifar10\n",
            "Training mode activated\n",
            "Dataset: cifar10\n",
            "Encode Length: 16\n",
            "Using CUDA\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 170498071/170498071 [00:01<00:00, 90476740.93it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|| 528M/528M [00:03<00:00, 175MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hparams: cifar10 --train --dataset cifar10 --encode_length 16 --lr 0.001 --batch_size 64 --epochs 200 --cuda --num_bad_epochs 6 --validate_frequency 20 --seed 8888 --device 0 --temperature 0.3 --weight 0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End of epoch   1 | loss   3.6298  | contra_loss   3.6266  | kl_loss   3.2314\n",
            "End of epoch   2 | loss   3.4592  | contra_loss   3.4554  | kl_loss   3.7798\n",
            "End of epoch   3 | loss   3.4102  | contra_loss   3.4067  | kl_loss   3.5199\n",
            "End of epoch   4 | loss   3.3783  | contra_loss   3.3747  | kl_loss   3.6317\n",
            "End of epoch   5 | loss   3.3549  | contra_loss   3.3515  | kl_loss   3.4163\n",
            "End of epoch   6 | loss   3.3403  | contra_loss   3.3369  | kl_loss   3.4887\n",
            "End of epoch   7 | loss   3.3181  | contra_loss   3.3146  | kl_loss   3.4518\n",
            "End of epoch   8 | loss   3.2981  | contra_loss   3.2947  | kl_loss   3.4474\n",
            "End of epoch   9 | loss   3.2762  | contra_loss   3.2728  | kl_loss   3.4128\n",
            "End of epoch  10 | loss   3.3014  | contra_loss   3.2979  | kl_loss   3.5601\n",
            "End of epoch  11 | loss   3.2925  | contra_loss   3.2891  | kl_loss   3.4637\n",
            "End of epoch  12 | loss   3.2551  | contra_loss   3.2518  | kl_loss   3.3701\n",
            "End of epoch  13 | loss   3.2611  | contra_loss   3.2578  | kl_loss   3.3550\n",
            "End of epoch  14 | loss   3.2436  | contra_loss   3.2402  | kl_loss   3.3987\n",
            "End of epoch  15 | loss   3.2422  | contra_loss   3.2388  | kl_loss   3.4148\n",
            "End of epoch  16 | loss   3.2545  | contra_loss   3.2511  | kl_loss   3.4652\n",
            "End of epoch  17 | loss   3.2444  | contra_loss   3.2410  | kl_loss   3.3863\n",
            "End of epoch  18 | loss   3.2417  | contra_loss   3.2383  | kl_loss   3.4411\n",
            "End of epoch  19 | loss   3.2382  | contra_loss   3.2349  | kl_loss   3.3273\n",
            "End of epoch  20 | loss   3.2633  | contra_loss   3.2599  | kl_loss   3.4248\n",
            "evaluating...\n",
            " | val perf   0.5496\t\t*Best model so far, deep copying*\n",
            "End of epoch  21 | loss   3.2406  | contra_loss   3.2372  | kl_loss   3.4133\n",
            "End of epoch  22 | loss   3.2153  | contra_loss   3.2120  | kl_loss   3.3643\n",
            "End of epoch  23 | loss   3.2217  | contra_loss   3.2183  | kl_loss   3.4337\n",
            "End of epoch  24 | loss   3.2317  | contra_loss   3.2283  | kl_loss   3.4001\n",
            "End of epoch  25 | loss   3.2149  | contra_loss   3.2115  | kl_loss   3.4298\n",
            "End of epoch  26 | loss   3.2196  | contra_loss   3.2162  | kl_loss   3.4460\n",
            "End of epoch  27 | loss   3.2105  | contra_loss   3.2072  | kl_loss   3.3470\n",
            "End of epoch  28 | loss   3.2150  | contra_loss   3.2115  | kl_loss   3.4261\n",
            "End of epoch  29 | loss   3.2035  | contra_loss   3.2001  | kl_loss   3.3797\n",
            "End of epoch  30 | loss   3.1851  | contra_loss   3.1817  | kl_loss   3.4355\n",
            "End of epoch  31 | loss   3.1950  | contra_loss   3.1917  | kl_loss   3.3517\n",
            "End of epoch  32 | loss   3.2102  | contra_loss   3.2068  | kl_loss   3.4252\n",
            "End of epoch  33 | loss   3.1976  | contra_loss   3.1942  | kl_loss   3.3920\n",
            "End of epoch  34 | loss   3.2022  | contra_loss   3.1989  | kl_loss   3.3268\n",
            "End of epoch  35 | loss   3.1965  | contra_loss   3.1931  | kl_loss   3.4399\n",
            "End of epoch  36 | loss   3.2086  | contra_loss   3.2052  | kl_loss   3.3742\n",
            "End of epoch  37 | loss   3.2011  | contra_loss   3.1977  | kl_loss   3.4531\n",
            "End of epoch  38 | loss   3.1971  | contra_loss   3.1938  | kl_loss   3.3429\n",
            "End of epoch  39 | loss   3.1816  | contra_loss   3.1782  | kl_loss   3.4465\n",
            "End of epoch  40 | loss   3.1999  | contra_loss   3.1965  | kl_loss   3.4366\n",
            "evaluating...\n",
            " | val perf   0.5621\t\t*Best model so far, deep copying*\n",
            "End of epoch  41 | loss   3.1939  | contra_loss   3.1904  | kl_loss   3.4685\n",
            "End of epoch  42 | loss   3.1891  | contra_loss   3.1857  | kl_loss   3.3903\n",
            "End of epoch  43 | loss   3.1837  | contra_loss   3.1804  | kl_loss   3.3725\n",
            "End of epoch  44 | loss   3.1613  | contra_loss   3.1579  | kl_loss   3.3982\n",
            "End of epoch  45 | loss   3.1716  | contra_loss   3.1682  | kl_loss   3.3684\n",
            "End of epoch  46 | loss   3.1751  | contra_loss   3.1717  | kl_loss   3.4017\n",
            "End of epoch  47 | loss   3.1586  | contra_loss   3.1552  | kl_loss   3.3544\n",
            "End of epoch  48 | loss   3.1718  | contra_loss   3.1684  | kl_loss   3.4568\n",
            "End of epoch  49 | loss   3.1771  | contra_loss   3.1737  | kl_loss   3.4572\n",
            "End of epoch  50 | loss   3.1839  | contra_loss   3.1805  | kl_loss   3.4003\n",
            "End of epoch  51 | loss   3.1697  | contra_loss   3.1664  | kl_loss   3.3477\n",
            "End of epoch  52 | loss   3.1514  | contra_loss   3.1481  | kl_loss   3.3241\n",
            "End of epoch  53 | loss   3.1382  | contra_loss   3.1348  | kl_loss   3.3500\n",
            "End of epoch  54 | loss   3.1474  | contra_loss   3.1441  | kl_loss   3.2967\n",
            "End of epoch  55 | loss   3.1717  | contra_loss   3.1683  | kl_loss   3.3804\n",
            "End of epoch  56 | loss   3.1579  | contra_loss   3.1546  | kl_loss   3.3625\n",
            "End of epoch  57 | loss   3.1617  | contra_loss   3.1584  | kl_loss   3.2910\n",
            "End of epoch  58 | loss   3.1661  | contra_loss   3.1627  | kl_loss   3.3490\n",
            "End of epoch  59 | loss   3.1514  | contra_loss   3.1480  | kl_loss   3.3849\n",
            "End of epoch  60 | loss   3.1536  | contra_loss   3.1502  | kl_loss   3.4118\n",
            "evaluating...\n",
            " | val perf   0.5682\t\t*Best model so far, deep copying*\n",
            "End of epoch  61 | loss   3.1516  | contra_loss   3.1482  | kl_loss   3.4033\n",
            "End of epoch  62 | loss   3.1291  | contra_loss   3.1258  | kl_loss   3.3259\n",
            "End of epoch  63 | loss   3.1556  | contra_loss   3.1523  | kl_loss   3.3351\n"
          ]
        }
      ],
      "source": [
        "args_list = ['cifar10', '--train', '--dataset', 'cifar10', '--encode_length', '16', '--cuda', '--device', '0', '--epochs', '200']\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('model_path', type=str, help='Path to the model or dataset name')\n",
        "    parser.add_argument('--train', action='store_true', help='Train the model')\n",
        "    parser.add_argument('--dataset', type=str, help='Name of the dataset', default='cifar10')\n",
        "    parser.add_argument('--encode_length', type=int, help='Length of encoding', default=16)\n",
        "    parser.add_argument('--cuda', action='store_true', help='Use CUDA for computation')\n",
        "    parser.add_argument('--lr', type=float, help='Learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, help='Batch size')\n",
        "    parser.add_argument('--epochs', type=int, help='Number of epochs')\n",
        "    parser.add_argument('--num_runs', type=int, help='Number of runs')\n",
        "    parser.add_argument('--num_bad_epochs', type=int, help='Number of bad epochs for early stopping')\n",
        "    parser.add_argument('--validate_frequency', type=int, help='Frequency of validation')\n",
        "    parser.add_argument('--num_workers', type=int, help='Number of workers')\n",
        "    parser.add_argument('--seed', type=int, help='Random seed')\n",
        "    parser.add_argument('--device', type=str, help='Device to use', default='0')\n",
        "    parser.add_argument('--temperature', type=float, help='Temperature for sampling')\n",
        "    parser.add_argument('--weight', type=float, help='Weight parameter')\n",
        "\n",
        "    args = parser.parse_args(args_list)\n",
        "\n",
        "    # Now you can use args to access the command-line arguments\n",
        "    print(f\"Model Path: {args.model_path}\")\n",
        "    if args.train:\n",
        "        print(\"Training mode activated\")\n",
        "    print(f\"Dataset: {args.dataset}\")\n",
        "    print(f\"Encode Length: {args.encode_length}\")\n",
        "    if args.cuda:\n",
        "        print(\"Using CUDA\")\n",
        "\n",
        "    # args_list\n",
        "    argparser = CIBHash.get_model_specific_argparser()\n",
        "    hparams = argparser.parse_args(args_list)\n",
        "\n",
        "    # CUDA\n",
        "    if hparams.cuda:\n",
        "        torch.cuda.set_device(hparams.device)\n",
        "\n",
        "    # \n",
        "    model = CIBHash(hparams)\n",
        "\n",
        "    # \n",
        "    if hparams.train:\n",
        "        model.run_training_sessions()\n",
        "    else:\n",
        "        model.load()\n",
        "        print('Loaded model with: %s' % model.flag_hparams())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
